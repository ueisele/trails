{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norwegian Trails Data Exploration\n",
    "\n",
    "**Goal**: Understand the actual structure and content of Turrutebasen data from Geonorge before building abstractions.\n",
    "\n",
    "**Data Source**: Geonorge/Kartverket - Norwegian government's official trail database (Turrutebasen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Our minimal modules\n",
    "from trails.analysis.describe import describe_dataframe\n",
    "from trails.io.sources.geonorge import Source as GeonorgeSource\n",
    "from trails.io.sources.language import Language\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Project paths\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "cache_dir = project_root / \".cache\"\n",
    "\n",
    "# Display settings\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Cache directory: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Geonorge Data\n",
    "\n",
    "First, let's load the data and see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Geonorge source and load data\n",
    "geonorge = GeonorgeSource(cache_dir=str(cache_dir))\n",
    "\n",
    "# Load with English translations (use Language.NO for Norwegian)\n",
    "trail_data = geonorge.load_turrutebasen(language=Language.EN)\n",
    "print(\"Data Source Information:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: {trail_data.metadata.dataset_name}\")\n",
    "print(f\"Provider: {trail_data.metadata.provider}\")\n",
    "print(f\"License: {trail_data.metadata.license}\")\n",
    "print(f\"Dataset ID: {trail_data.metadata.dataset_id}\")\n",
    "print(f\"Description: {trail_data.metadata.description}\")\n",
    "print(f\"Attribution: {trail_data.metadata.attribution}\")\n",
    "\n",
    "print(f\"\\nData loaded successfully from: {trail_data.source_url}\")\n",
    "print(f\"Version: {trail_data.version}\")\n",
    "print(f\"Language: {trail_data.language.value}\")\n",
    "print(f\"CRS: {trail_data.crs}\")\n",
    "print(f\"Total features: {trail_data.total_features}\")\n",
    "\n",
    "# Layers and columns are now automatically translated\n",
    "print(f\"\\nFound {len(trail_data.spatial_layers)} spatial layers:\")\n",
    "for layer_name, gdf in trail_data.spatial_layers.items():\n",
    "    print(f\"  - {layer_name}: {len(gdf)} features\")\n",
    "\n",
    "print(f\"\\nFound {len(trail_data.attribute_tables)} attribute tables:\")\n",
    "for table_name, df in trail_data.attribute_tables.items():\n",
    "    print(f\"  - {table_name}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Layer Analysis\n",
    "\n",
    "Let's examine each layer in detail, analyzing both structure and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete analysis of all layers using describe_dataframe\n",
    "print(\"=\" * 80)\n",
    "print(\"SPATIAL LAYERS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "layer_summary = {}\n",
    "\n",
    "for layer_name, gdf in trail_data.spatial_layers.items():\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"LAYER: {layer_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Basic info\n",
    "    print(\"\\nBasic Information:\")\n",
    "    print(f\"  Shape: {gdf.shape}\")\n",
    "    print(f\"  CRS: {gdf.crs}\")\n",
    "\n",
    "    # Geometry info\n",
    "    print(\"\\nGeometry Information:\")\n",
    "    if not gdf.empty and \"geometry\" in gdf.columns:\n",
    "        geom_types = gdf.geometry.geom_type.unique()\n",
    "        print(f\"  Geometry types: {geom_types}\")\n",
    "        print(f\"  Total bounds: {gdf.total_bounds}\")\n",
    "\n",
    "    # Column analysis using describe_dataframe (sorted by completeness)\n",
    "    columns = [col for col in gdf.columns if col != \"geometry\"]\n",
    "    print(f\"\\nColumn Analysis ({len(columns)} columns, sorted by completeness):\")\n",
    "\n",
    "    # Use describe_dataframe for detailed analysis with automatic sorting\n",
    "    print(\"\\n\" + describe_dataframe(gdf, columns))\n",
    "\n",
    "    # Store summary\n",
    "    layer_summary[layer_name] = {\n",
    "        \"shape\": gdf.shape,\n",
    "        \"crs\": str(gdf.crs),\n",
    "        \"columns\": list(gdf.columns),\n",
    "        \"geometry_types\": list(geom_types) if not gdf.empty else [],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ATTRIBUTE TABLES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for table_name, df in trail_data.attribute_tables.items():\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"TABLE: {table_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Basic info\n",
    "    print(\"\\nBasic Information:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "\n",
    "    # Column analysis using describe_dataframe (sorted by completeness)\n",
    "    print(f\"\\nColumn Analysis ({len(df.columns)} columns, sorted by completeness):\")\n",
    "\n",
    "    # Use describe_dataframe for detailed analysis with automatic sorting\n",
    "    print(\"\\n\" + describe_dataframe(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify Trail Layers\n",
    "\n",
    "Let's identify which layers contain actual trail data (LineString geometries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find trail layers (with line geometries)\n",
    "trail_layers = {}\n",
    "point_layers = {}\n",
    "other_layers = {}\n",
    "\n",
    "for layer_name, gdf in trail_data.spatial_layers.items():\n",
    "    if gdf.empty:\n",
    "        other_layers[layer_name] = gdf\n",
    "        continue\n",
    "\n",
    "    # Check geometry type\n",
    "    geom_types = gdf.geometry.geom_type.unique()\n",
    "\n",
    "    if any(gt in [\"LineString\", \"MultiLineString\"] for gt in geom_types):\n",
    "        trail_layers[layer_name] = gdf\n",
    "    elif any(gt in [\"Point\", \"MultiPoint\"] for gt in geom_types):\n",
    "        point_layers[layer_name] = gdf\n",
    "    else:\n",
    "        other_layers[layer_name] = gdf\n",
    "\n",
    "print(\"Layer Classification:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTrail Layers (Lines): {len(trail_layers)}\")\n",
    "for name, gdf in trail_layers.items():\n",
    "    print(f\"  - {name}: {len(gdf)} features\")\n",
    "\n",
    "print(f\"\\nPoint Layers (Facilities): {len(point_layers)}\")\n",
    "for name, gdf in point_layers.items():\n",
    "    print(f\"  - {name}: {len(gdf)} features\")\n",
    "\n",
    "if other_layers:\n",
    "    print(f\"\\nOther Layers: {len(other_layers)}\")\n",
    "    for name, gdf in other_layers.items():\n",
    "        print(f\"  - {name}: {len(gdf)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Structure & Relationships\n",
    "\n",
    "Let's understand how the hiking trail data is organized in the database, including spatial layers, attribute tables, and their relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Spatial Layer Schema\n",
    "\n",
    "Analyze the structure and fields of the spatial layer containing trail geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze spatial layer schema for hiking trails\n",
    "hiking_trail_layer = \"hiking_trail_centerline\" if trail_data.language == Language.EN else \"fotrute_senterlinje\"\n",
    "\n",
    "if hiking_trail_layer in trail_data.spatial_layers:\n",
    "    trails = trail_data.spatial_layers[hiking_trail_layer]\n",
    "\n",
    "    print(\"SPATIAL LAYER SCHEMA ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Basic schema info\n",
    "    print(\"\\n1. LAYER STRUCTURE\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Layer name: {hiking_trail_layer}\")\n",
    "    print(f\"Total features: {len(trails):,}\")\n",
    "    print(f\"CRS: {trails.crs}\")\n",
    "    print(f\"Geometry type: {trails.geometry.geom_type.unique()}\")\n",
    "    print(f\"Total columns: {len(trails.columns)}\")\n",
    "\n",
    "    # Analyze column types\n",
    "    print(\"\\n2. COLUMN TYPES\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Group columns by data type\n",
    "    spatial_type_groups: dict[str, list[str]] = {}\n",
    "    for col in trails.columns:\n",
    "        if col != \"geometry\":\n",
    "            dtype_str = str(trails[col].dtype)\n",
    "            if dtype_str not in spatial_type_groups:\n",
    "                spatial_type_groups[dtype_str] = []\n",
    "            spatial_type_groups[dtype_str].append(col)\n",
    "\n",
    "    for dtype_str, cols in sorted(spatial_type_groups.items()):\n",
    "        print(f\"\\n{dtype_str} columns ({len(cols)}):\")\n",
    "        for col in sorted(cols):\n",
    "            print(f\"  - {col}\")\n",
    "\n",
    "    # Identify key fields using describe_dataframe (no sorting)\n",
    "    print(\"\\n3. KEY IDENTIFIER FIELDS\")\n",
    "    print(\"-\" * 50)\n",
    "    print(describe_dataframe(trails, [\"local_id\", \"version_id\", \"namespace\"], sort_by_completeness=False))\n",
    "\n",
    "    # Metadata fields using describe_dataframe (no sorting)\n",
    "    print(\"\\n4. METADATA FIELDS\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\n",
    "        describe_dataframe(\n",
    "            trails, [\"origin\", \"update_date\", \"copy_date\", \"data_capture_date\", \"measurement_method\", \"accuracy\"], sort_by_completeness=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Trail attribute fields using describe_dataframe (with sorting)\n",
    "    print(\"\\n5. TRAIL ATTRIBUTE FIELDS (in spatial layer)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\n",
    "        describe_dataframe(\n",
    "            trails,\n",
    "            [\n",
    "                \"object_type\",\n",
    "                \"marking\",\n",
    "                \"signage\",\n",
    "                \"lighting\",\n",
    "                \"season\",\n",
    "                \"information\",\n",
    "                \"trail_follows\",\n",
    "                \"trail_width\",\n",
    "                \"surface_type\",\n",
    "                \"traffic_load\",\n",
    "                \"SHAPE_Length\",\n",
    "            ],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Attribute Table Schema\n",
    "\n",
    "Analyze the structure of the attribute table that contains detailed trail information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attribute table schema\n",
    "hiking_attrs_layer = \"hiking_trail_info_table\" if trail_data.language == Language.EN else \"fotrute_info_tabell\"\n",
    "\n",
    "if hiking_attrs_layer in trail_data.attribute_tables:\n",
    "    hiking_attrs = trail_data.attribute_tables[hiking_attrs_layer]\n",
    "\n",
    "    print(\"ATTRIBUTE TABLE SCHEMA ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Basic schema info\n",
    "    print(\"\\n1. TABLE STRUCTURE\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Table name: {hiking_attrs_layer}\")\n",
    "    print(f\"Total rows: {len(hiking_attrs):,}\")\n",
    "    print(f\"Total columns: {len(hiking_attrs.columns)}\")\n",
    "\n",
    "    # Column types\n",
    "    print(\"\\n2. COLUMN TYPES\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    attr_type_groups: dict[str, list[str]] = {}\n",
    "    for col in hiking_attrs.columns:\n",
    "        dtype_str = str(hiking_attrs[col].dtype)\n",
    "        if dtype_str not in attr_type_groups:\n",
    "            attr_type_groups[dtype_str] = []\n",
    "        attr_type_groups[dtype_str].append(col)\n",
    "\n",
    "    for dtype_str, cols in sorted(attr_type_groups.items()):\n",
    "        print(f\"\\n{dtype_str} columns ({len(cols)}):\")\n",
    "        for col in sorted(cols):\n",
    "            print(f\"  - {col}\")\n",
    "\n",
    "    # Key fields using describe_dataframe\n",
    "    print(\"\\n3. KEY FIELDS\")\n",
    "    print(\"-\" * 50)\n",
    "    print(describe_dataframe(hiking_attrs, [\"hiking_trail_fk\"], sort_by_completeness=False))\n",
    "\n",
    "    # Information fields using describe_dataframe\n",
    "    print(\"\\n4. INFORMATION FIELDS\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\n",
    "        describe_dataframe(\n",
    "            hiking_attrs,\n",
    "            [\n",
    "                \"object_type\",\n",
    "                \"special_hiking_trail_type\",\n",
    "                \"trail_type\",\n",
    "                \"trail_name\",\n",
    "                \"trail_number\",\n",
    "                \"trail_significance\",\n",
    "                \"trail_information\",\n",
    "                \"accessibility\",\n",
    "                \"difficulty\",\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Administrative fields using describe_dataframe\n",
    "    print(\"\\n5. ADMINISTRATIVE FIELDS\")\n",
    "    print(\"-\" * 50)\n",
    "    print(describe_dataframe(hiking_attrs, [\"maintenance_responsible\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Foreign Key Relationships\n",
    "\n",
    "Analyze how the spatial layer and attribute table are connected through foreign keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze foreign key relationships\n",
    "if \"hiking_trail_fk\" in hiking_attrs.columns:\n",
    "    print(\"FOREIGN KEY RELATIONSHIP ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    hiking_spatial = trail_data.spatial_layers[hiking_trail_layer]\n",
    "\n",
    "    # Basic relationship stats\n",
    "    print(\"\\n1. RELATIONSHIP OVERVIEW\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Spatial layer records: {len(hiking_spatial):,}\")\n",
    "    print(f\"Attribute table records: {len(hiking_attrs):,}\")\n",
    "    print(\"Relationship type: Many-to-One (many attribute rows -> one geometry)\")\n",
    "\n",
    "    # Analyze FK distribution\n",
    "    fk_counts = hiking_attrs[\"hiking_trail_fk\"].value_counts()\n",
    "\n",
    "    print(\"\\n2. FOREIGN KEY DISTRIBUTION\")\n",
    "    print(\"-\" * 50)\n",
    "    fk_dtype = hiking_attrs[\"hiking_trail_fk\"].dtype\n",
    "    print(f\"Foreign key column type: hiking_trail_fk [{fk_dtype}]\")\n",
    "    print(f\"Unique foreign keys: {hiking_attrs['hiking_trail_fk'].nunique():,}\")\n",
    "    print(f\"Should match spatial records: {len(hiking_spatial):,}\")\n",
    "    print(f\"Match: {'✓ Yes' if hiking_attrs['hiking_trail_fk'].nunique() == len(hiking_spatial) else '✗ No'}\")\n",
    "\n",
    "    print(\"\\nDistribution of attribute rows per geometry:\")\n",
    "    print(f\"  Min: {fk_counts.min()} row(s)\")\n",
    "    print(f\"  Max: {fk_counts.max()} row(s)\")\n",
    "    print(f\"  Mean: {fk_counts.mean():.2f} row(s)\")\n",
    "    print(f\"  Median: {fk_counts.median():.0f} row(s)\")\n",
    "\n",
    "    # Detailed distribution\n",
    "    print(\"\\n3. RELATIONSHIP CARDINALITY\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    cardinality_dist = fk_counts.value_counts().sort_index()\n",
    "\n",
    "    # Group into categories\n",
    "    one_to_one = (fk_counts == 1).sum()\n",
    "    one_to_two = (fk_counts == 2).sum()\n",
    "    one_to_many = (fk_counts > 2).sum()\n",
    "\n",
    "    print(f\"One-to-One (1 attribute row): {one_to_one:,} geometries ({one_to_one / len(fk_counts) * 100:.1f}%)\")\n",
    "    print(f\"One-to-Two (2 attribute rows): {one_to_two:,} geometries ({one_to_two / len(fk_counts) * 100:.1f}%)\")\n",
    "    print(f\"One-to-Many (3+ attribute rows): {one_to_many:,} geometries ({one_to_many / len(fk_counts) * 100:.1f}%)\")\n",
    "\n",
    "    # Show distribution for first 20 cardinalities\n",
    "    print(\"\\nDetailed cardinality distribution:\")\n",
    "    for n_rows, count in cardinality_dist.head(20).items():\n",
    "        pct = count / len(fk_counts) * 100\n",
    "        print(f\"  {n_rows} attribute row(s): {count:,} geometries ({pct:.1f}%)\")\n",
    "\n",
    "    # Analyze why multiple rows exist\n",
    "    print(\"\\n4. MULTIPLE ROWS ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Get FKs with multiple rows\n",
    "    multi_row_fks = fk_counts[fk_counts > 1].index\n",
    "\n",
    "    if len(multi_row_fks) > 0:\n",
    "        # Sample a FK with multiple rows\n",
    "        sample_fk = multi_row_fks[0]\n",
    "        sample_rows = hiking_attrs[hiking_attrs[\"hiking_trail_fk\"] == sample_fk]\n",
    "\n",
    "        print(f\"Example: FK '{sample_fk[:8]}...' has {len(sample_rows)} rows\")\n",
    "\n",
    "        # Check what differs between rows\n",
    "        varying_cols = []\n",
    "        for col in hiking_attrs.columns:\n",
    "            if col != \"hiking_trail_fk\" and sample_rows[col].nunique() > 1:\n",
    "                varying_cols.append(col)\n",
    "\n",
    "        print(f\"Columns that vary across these rows: {len(varying_cols)}\")\n",
    "        if \"trail_name\" in varying_cols:\n",
    "            name_dtype = hiking_attrs[\"trail_name\"].dtype\n",
    "            print(f\"\\nDifferent trail names [column type: {name_dtype}] for same segment:\")\n",
    "            for name in sample_rows[\"trail_name\"].unique()[:5]:\n",
    "                if pd.notna(name):\n",
    "                    print(f\"  - {name}\")\n",
    "\n",
    "    # Validation check\n",
    "    print(\"\\n5. DATA INTEGRITY CHECKS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Check if all FKs in attribute table exist in spatial layer\n",
    "    spatial_ids = set(hiking_spatial[\"local_id\"])\n",
    "    attr_fks = set(hiking_attrs[\"hiking_trail_fk\"])\n",
    "\n",
    "    # Get types for comparison\n",
    "    spatial_id_dtype = hiking_spatial[\"local_id\"].dtype\n",
    "    attr_fk_dtype = hiking_attrs[\"hiking_trail_fk\"].dtype\n",
    "\n",
    "    print(f\"Comparing: local_id [{spatial_id_dtype}] with hiking_trail_fk [{attr_fk_dtype}]\")\n",
    "\n",
    "    orphaned_fks = attr_fks - spatial_ids\n",
    "    print(f\"Orphaned foreign keys (in attributes but not spatial): {len(orphaned_fks)}\")\n",
    "\n",
    "    unreferenced_ids = spatial_ids - attr_fks\n",
    "    print(f\"Unreferenced geometries (in spatial but not attributes): {len(unreferenced_ids)}\")\n",
    "\n",
    "    if len(orphaned_fks) == 0 and len(unreferenced_ids) == 0:\n",
    "        print(\"✓ Perfect referential integrity - all relationships are valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Data Quality Assessment\n",
    "\n",
    "Evaluate the completeness and quality of the data across both layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Spatial layer quality\n",
    "print(\"\\n1. SPATIAL LAYER QUALITY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Geometry validation:\")\n",
    "print(f\"  Valid geometries: {trails.geometry.is_valid.sum():,} / {len(trails):,}\")\n",
    "print(f\"  Invalid geometries: {(~trails.geometry.is_valid).sum()}\")\n",
    "print(f\"  Empty geometries: {trails.geometry.is_empty.sum()}\")\n",
    "print(f\"  Simple geometries: {trails.geometry.is_simple.sum():,} / {len(trails):,}\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_geoms = trails.geometry.duplicated().sum()\n",
    "print(f\"  Duplicate geometries: {duplicate_geoms}\")\n",
    "\n",
    "# Field completeness in spatial layer\n",
    "print(\"\\nField completeness (spatial layer):\")\n",
    "spatial_completeness = []\n",
    "for col in trails.columns:\n",
    "    if col != \"geometry\":\n",
    "        dtype = trails[col].dtype\n",
    "        non_null = trails[col].notna().sum()\n",
    "        completeness = (non_null / len(trails)) * 100\n",
    "        spatial_completeness.append((col, dtype, completeness))\n",
    "\n",
    "# Sort by completeness\n",
    "spatial_completeness.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"  Fully complete fields (100%):\")\n",
    "for col, dtype, pct in spatial_completeness:\n",
    "    if pct == 100:\n",
    "        print(f\"    - {col} [{dtype}]\")\n",
    "\n",
    "print(\"  Partially populated fields (<100% & >=10%):\")\n",
    "partial_fields = [(col, dtype, pct) for col, dtype, pct in spatial_completeness if 100 > pct >= 10]\n",
    "for col, dtype, pct in partial_fields:\n",
    "    print(f\"    - {col} [{dtype}]: {pct:.1f}%\")\n",
    "\n",
    "print(\"  Poorly populated fields (<10%):\")\n",
    "poor_fields = [(col, dtype, pct) for col, dtype, pct in spatial_completeness if pct < 10]\n",
    "for col, dtype, pct in poor_fields:\n",
    "    print(f\"    - {col} [{dtype}]: {pct:.1f}%\")\n",
    "\n",
    "# Attribute table quality\n",
    "print(\"\\n2. ATTRIBUTE TABLE QUALITY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Critical fields completeness\n",
    "critical_fields = [\"trail_name\", \"trail_number\", \"difficulty\", \"trail_type\", \"trail_significance\", \"maintenance_responsible\", \"hiking_trail_fk\"]\n",
    "\n",
    "print(\"Critical field completeness:\")\n",
    "for field in critical_fields:\n",
    "    if field in hiking_attrs.columns:\n",
    "        dtype = hiking_attrs[field].dtype\n",
    "        non_null = hiking_attrs[field].notna().sum()\n",
    "        completeness = (non_null / len(hiking_attrs)) * 100\n",
    "        print(f\"  {field} [{dtype}]: {completeness:.1f}%\")\n",
    "\n",
    "# Check for unnamed trails\n",
    "if \"trail_name\" in hiking_attrs.columns:\n",
    "    name_dtype = hiking_attrs[\"trail_name\"].dtype\n",
    "    unnamed = hiking_attrs[\"trail_name\"].isna().sum()\n",
    "    unknown = (hiking_attrs[\"trail_name\"] == \"Ukjent\").sum() if \"Ukjent\" in hiking_attrs[\"trail_name\"].values else 0\n",
    "    total_unnamed = unnamed + unknown\n",
    "    print(f\"\\nUnnamed/unknown trails in trail_name [{name_dtype}]: {total_unnamed:,} ({total_unnamed / len(hiking_attrs) * 100:.1f}%)\")\n",
    "\n",
    "# Overall assessment\n",
    "print(\"\\n3. OVERALL DATA QUALITY SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate quality score including new metrics\n",
    "quality_metrics = {\n",
    "    \"Geometry validity\": trails.geometry.is_valid.sum() / len(trails) * 100,\n",
    "    \"No duplicate geometries\": (len(trails) - duplicate_geoms) / len(trails) * 100,\n",
    "    \"Trail names present\": (len(hiking_attrs) - total_unnamed) / len(hiking_attrs) * 100 if \"trail_name\" in hiking_attrs.columns else 0,\n",
    "    \"Trail numbers present\": hiking_attrs[\"trail_number\"].notna().sum() / len(hiking_attrs) * 100 if \"trail_number\" in hiking_attrs.columns else 0,\n",
    "    \"Maintenance responsible present\": hiking_attrs[\"maintenance_responsible\"].notna().sum() / len(hiking_attrs) * 100\n",
    "    if \"maintenance_responsible\" in hiking_attrs.columns\n",
    "    else 0,\n",
    "    \"Referential integrity\": 100 if len(orphaned_fks) == 0 and len(unreferenced_ids) == 0 else 0,\n",
    "}\n",
    "\n",
    "print(\"Quality metrics:\")\n",
    "for metric, score in quality_metrics.items():\n",
    "    status = \"✓\" if score >= 95 else \"⚠\" if score >= 80 else \"✗\"\n",
    "    print(f\"  {status} {metric}: {score:.1f}%\")\n",
    "\n",
    "avg_quality = sum(quality_metrics.values()) / len(quality_metrics)\n",
    "print(f\"\\nOverall quality score: {avg_quality:.1f}%\")\n",
    "\n",
    "# Data structure summary\n",
    "print(\"\\n4. DATA STRUCTURE INSIGHTS\")\n",
    "print(\"-\" * 50)\n",
    "print(\"✓ Normalized database design prevents geometry duplication\")\n",
    "print(\"✓ Many-to-one relationships allow multiple trail names per segment\")\n",
    "print(\"✓ Spatial layer contains minimal attributes (optimized for geometry)\")\n",
    "print(\"✓ Attribute table contains rich trail information\")\n",
    "print(f\"✓ Total unique trail segments: {len(trails):,}\")\n",
    "print(f\"✓ Total trail name assignments: {len(hiking_attrs):,}\")\n",
    "print(f\"✓ Segments with multiple names: {len(multi_row_fks):,} ({len(multi_row_fks) / len(trails) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trail Analysis\n",
    "\n",
    "Now let's analyze what the data tells us about the trails themselves - their characteristics, distribution, and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Spatial Distribution\n",
    "\n",
    "Analyze where the trails are located and their spatial coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze spatial distribution for main trail layer\n",
    "hiking_trail_layer = \"hiking_trail_centerline\" if trail_data.language == Language.EN else \"fotrute_senterlinje\"\n",
    "\n",
    "if hiking_trail_layer in trail_data.spatial_layers:\n",
    "    trails = trail_data.spatial_layers[hiking_trail_layer]\n",
    "\n",
    "    print(\"Spatial Distribution Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Get bounds\n",
    "    bounds = trails.total_bounds\n",
    "    print(\"\\nSpatial extent (minx, miny, maxx, maxy):\")\n",
    "    print(f\"  {bounds}\")\n",
    "    print(f\"\\nCRS: {trails.crs}\")\n",
    "\n",
    "    # Analyze by object_type if it exists (translated column name)\n",
    "    objtype_col = \"object_type\" if trail_data.language == Language.EN else \"objtype\"\n",
    "    if objtype_col in trails.columns:\n",
    "        print(f\"\\nDistribution by {objtype_col}:\")\n",
    "        type_stats = trails[objtype_col].value_counts()\n",
    "        for trail_type, count in type_stats.items():\n",
    "            # Values are already expanded and translated\n",
    "            print(f\"  {trail_type}: {count} trails ({count / len(trails) * 100:.1f}%)\")\n",
    "\n",
    "    # Use SHAPE_Length directly (already in meters for EPSG:25833)\n",
    "    print(\"\\nLength Analysis:\")\n",
    "\n",
    "    # SHAPE_Length is already in meters for this projected CRS\n",
    "    total_length = trails[\"SHAPE_Length\"].sum()\n",
    "    avg_length = trails[\"SHAPE_Length\"].mean()\n",
    "    median_length = trails[\"SHAPE_Length\"].median()\n",
    "\n",
    "    # Print in kilometers for readability\n",
    "    print(f\"  Total length: {total_length / 1000:,.1f} km\")\n",
    "    print(f\"  Average trail length: {avg_length / 1000:.2f} km\")\n",
    "    print(f\"  Median trail length: {median_length / 1000:.2f} km\")\n",
    "\n",
    "    # Show distribution of trail lengths\n",
    "    print(\"\\nTrail Length Distribution:\")\n",
    "    print(f\"  Shortest trail: {trails['SHAPE_Length'].min() / 1000:.3f} km\")\n",
    "    print(f\"  Longest trail: {trails['SHAPE_Length'].max() / 1000:.1f} km\")\n",
    "\n",
    "    # Length categories (using meters for calculation, displaying in km)\n",
    "    bins = [0, 1000, 5000, 10000, 20000, float(\"inf\")]\n",
    "    labels = [\"<1km\", \"1-5km\", \"5-10km\", \"10-20km\", \">20km\"]\n",
    "    trails[\"length_category\"] = pd.cut(trails[\"SHAPE_Length\"], bins=bins, labels=labels)\n",
    "\n",
    "    print(\"\\nTrails by length category:\")\n",
    "    for category in labels:\n",
    "        count = (trails[\"length_category\"] == category).sum()\n",
    "        pct = count / len(trails) * 100\n",
    "        print(f\"  {category}: {count:,} trails ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Trail Names & Types\n",
    "\n",
    "Analyze trail naming patterns and classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trail names and types analysis\n",
    "from typing import Any\n",
    "\n",
    "print(\"TRAIL NAMES, TYPES AND INFRASTRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Trail names\n",
    "print(\"\\n1. TRAIL NAMES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"trail_name\" in hiking_attrs.columns:\n",
    "    name_dtype = hiking_attrs[\"trail_name\"].dtype\n",
    "    unique_names = hiking_attrs[\"trail_name\"].nunique()\n",
    "    total_named = hiking_attrs[\"trail_name\"].notna().sum()\n",
    "    unnamed = hiking_attrs[\"trail_name\"].isna().sum()\n",
    "\n",
    "    print(f\"Column: trail_name [{name_dtype}]\")\n",
    "    print(f\"Unique trail names: {unique_names:,}\")\n",
    "    print(f\"Named trail segments: {total_named:,} ({total_named / len(hiking_attrs) * 100:.1f}%)\")\n",
    "    print(f\"Unnamed segments: {unnamed:,} ({unnamed / len(hiking_attrs) * 100:.1f}%)\")\n",
    "\n",
    "    # Most common trail names\n",
    "    name_counts = hiking_attrs[\"trail_name\"].value_counts()\n",
    "\n",
    "    print(\"\\nTop 15 most common trail names:\")\n",
    "    # Iterate without unpacking to avoid type issues\n",
    "    for _i, item in enumerate(name_counts.head(15).items()):\n",
    "        trail_name_val: Any = item[0]  # Type hint to avoid mypy error\n",
    "        trail_count: int = item[1]\n",
    "        pct = trail_count / len(hiking_attrs) * 100\n",
    "        print(f\"  {trail_name_val}: {trail_count:,} segments ({pct:.2f}%)\")\n",
    "\n",
    "    # Trail name patterns\n",
    "    print(\"\\n2. TRAIL NAME PATTERNS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Check for numbered trails\n",
    "    numbered_pattern = hiking_attrs[\"trail_name\"].str.contains(r\"\\d+\", na=False).sum()\n",
    "    print(f\"Trails with numbers: {numbered_pattern:,}\")\n",
    "\n",
    "    # Check for common prefixes/patterns\n",
    "    common_patterns = {\n",
    "        \"Pilegrimsleden\": hiking_attrs[\"trail_name\"].str.contains(\"Pilegrimsleden\", na=False).sum(),\n",
    "        \"Kyststi\": hiking_attrs[\"trail_name\"].str.contains(\"Kyststi\", na=False).sum(),\n",
    "        \"DNT\": hiking_attrs[\"trail_name\"].str.contains(\"DNT\", na=False).sum(),\n",
    "        \"Tursti\": hiking_attrs[\"trail_name\"].str.contains(\"Tursti\", na=False).sum(),\n",
    "        \"Rundtur\": hiking_attrs[\"trail_name\"].str.contains(\"Rundtur\", na=False).sum(),\n",
    "    }\n",
    "\n",
    "    print(\"Common trail name patterns:\")\n",
    "    for pattern, pattern_count in common_patterns.items():\n",
    "        if pattern_count > 0:\n",
    "            pct = pattern_count / total_named * 100\n",
    "            print(f\"  Contains '{pattern}': {pattern_count:,} segments ({pct:.2f}%)\")\n",
    "\n",
    "# Trail types\n",
    "print(\"\\n3. TRAIL TYPES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"trail_type\" in hiking_attrs.columns:\n",
    "    type_dtype = hiking_attrs[\"trail_type\"].dtype\n",
    "    type_counts = hiking_attrs[\"trail_type\"].value_counts()\n",
    "\n",
    "    print(f\"Column: trail_type [{type_dtype}]\")\n",
    "    print(\"Trail type distribution:\")\n",
    "    for trail_type, type_count in type_counts.items():\n",
    "        pct = type_count / len(hiking_attrs) * 100\n",
    "        print(f\"  {trail_type}: {type_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Special hiking trail types\n",
    "if \"special_hiking_trail_type\" in hiking_attrs.columns:\n",
    "    print(\"\\n4. SPECIAL TRAIL TYPES\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    special_dtype = hiking_attrs[\"special_hiking_trail_type\"].dtype\n",
    "    special_counts = hiking_attrs[\"special_hiking_trail_type\"].value_counts()\n",
    "\n",
    "    print(f\"Column: special_hiking_trail_type [{special_dtype}]\")\n",
    "    print(\"Special hiking trail classifications:\")\n",
    "    for special_type, special_count in special_counts.items():\n",
    "        pct = special_count / len(hiking_attrs) * 100\n",
    "        print(f\"  {special_type}: {special_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Trail significance\n",
    "if \"trail_significance\" in hiking_attrs.columns:\n",
    "    print(\"\\n5. TRAIL SIGNIFICANCE\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    sig_dtype = hiking_attrs[\"trail_significance\"].dtype\n",
    "    sig_counts = hiking_attrs[\"trail_significance\"].value_counts()\n",
    "\n",
    "    print(f\"Column: trail_significance [{sig_dtype}]\")\n",
    "    print(\"Trail significance levels:\")\n",
    "    for significance, sig_count in sig_counts.items():\n",
    "        pct = sig_count / len(hiking_attrs) * 100\n",
    "        print(f\"  {significance}: {sig_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Marking and signage (from spatial layer)\n",
    "print(\"\\n5. TRAIL MAINTENANCE\")\n",
    "print(\"-\" * 50)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Maintenance Responsible\n",
    "if \"maintenance_responsible\" in hiking_attrs.columns:\n",
    "    print(\"\\n6. MAINTENANCE RESPONSIBLE\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    maint_dtype = hiking_attrs[\"maintenance_responsible\"].dtype\n",
    "    maint_counts = hiking_attrs[\"maintenance_responsible\"].value_counts()\n",
    "    maint_total = hiking_attrs[\"maintenance_responsible\"].notna().sum()\n",
    "\n",
    "    print(f\"Column: maintenance_responsible [{maint_dtype}]\")\n",
    "    print(f\"Trails with maintenance info: {maint_total:,} ({maint_total / len(hiking_attrs) * 100:.1f}%)\")\n",
    "    print(f\"Unique maintenance organizations: {hiking_attrs['maintenance_responsible'].nunique():,}\")\n",
    "\n",
    "    print(\"\\nTop 10 maintenance organizations:\")\n",
    "    for _i, item in enumerate(maint_counts.head(10).items()):\n",
    "        org_name: Any = item[0]  # Type hint to avoid mypy error\n",
    "        org_count: int = item[1]\n",
    "        pct = org_count / len(hiking_attrs) * 100\n",
    "        print(f\"  {org_name}: {org_count:,} segments ({pct:.1f}%)\")\n",
    "\n",
    "    # Check for common organization types\n",
    "    print(\"\\nOrganization type patterns:\")\n",
    "    org_patterns = {\n",
    "        \"DNT\": hiking_attrs[\"maintenance_responsible\"].str.contains(\"DNT|Turistforening\", na=False).sum(),\n",
    "        \"Kommune\": hiking_attrs[\"maintenance_responsible\"].str.contains(\"kommune|Kommune\", na=False).sum(),\n",
    "        \"Nasjonalt pilegrimssenter\": hiking_attrs[\"maintenance_responsible\"].str.contains(\"Nasjonalt pilegrimssenter\", na=False).sum(),\n",
    "        \"Idrettslag\": hiking_attrs[\"maintenance_responsible\"].str.contains(\"idrettslag|IL|Idrettslag\", na=False).sum(),\n",
    "        \"Turlag\": hiking_attrs[\"maintenance_responsible\"].str.contains(\"turlag|Turlag\", na=False).sum(),\n",
    "    }\n",
    "\n",
    "    for pattern, pattern_total in org_patterns.items():\n",
    "        if pattern_total > 0:\n",
    "            pct = pattern_total / maint_total * 100\n",
    "            print(f\"  Contains '{pattern}': {pattern_total:,} trails ({pct:.1f}%)\")\n",
    "\n",
    "# Marking and signage (from spatial layer)\n",
    "print(\"\\n5. TRAIL INFRASTRUCTURE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"marking\" in trails.columns:\n",
    "    marked = trails[\"marking\"].notna().sum()\n",
    "    print(f\"Segments with marking info: {marked:,} ({marked / len(trails) * 100:.1f}%)\")\n",
    "\n",
    "    if marked > 0:\n",
    "        mark_values = trails[\"marking\"].value_counts()\n",
    "        for mark_type, mark_count in mark_values.items():\n",
    "            pct = mark_count / len(trails) * 100\n",
    "            print(f\"  {mark_type}: {mark_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "if \"signage\" in trails.columns:\n",
    "    signage = trails[\"signage\"].notna().sum()\n",
    "    print(f\"\\nSegments with signage info: {signage:,} ({signage / len(trails) * 100:.1f}%)\")\n",
    "\n",
    "    if signage > 0:\n",
    "        sign_values = trails[\"signage\"].value_counts()\n",
    "        for sign_type, sign_count in sign_values.items():\n",
    "            pct = sign_count / len(trails) * 100\n",
    "            print(f\"  {sign_type}: {sign_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "if \"lighting\" in trails.columns:\n",
    "    lighting = trails[\"lighting\"].notna().sum()\n",
    "    print(f\"\\nSegments with lighting: {lighting:,} ({lighting / len(trails) * 100:.1f}%)\")\n",
    "\n",
    "    if lighting > 0:\n",
    "        light_values = trails[\"lighting\"].value_counts()\n",
    "        for light_type, light_count in light_values.items():\n",
    "            pct = light_count / len(trails) * 100\n",
    "            print(f\"  {light_type}: {light_count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Difficulty & Classification\n",
    "\n",
    "Analyze trail difficulty ratings and user classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difficulty and classification analysis\n",
    "print(\"DIFFICULTY & CLASSIFICATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Difficulty distribution\n",
    "print(\"\\n1. DIFFICULTY RATINGS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"difficulty\" in hiking_attrs.columns:\n",
    "    diff_counts = hiking_attrs[\"difficulty\"].value_counts()\n",
    "    diff_total = hiking_attrs[\"difficulty\"].notna().sum()\n",
    "\n",
    "    print(f\"Trails with difficulty rating: {diff_total:,} ({diff_total / len(hiking_attrs) * 100:.1f}%)\")\n",
    "    print(f\"Missing difficulty rating: {len(hiking_attrs) - diff_total:,}\")\n",
    "\n",
    "    print(\"\\nDifficulty distribution:\")\n",
    "    for difficulty, count in diff_counts.items():\n",
    "        pct = count / len(hiking_attrs) * 100\n",
    "        pct_rated = count / diff_total * 100\n",
    "        print(f\"  {difficulty}: {count:,} ({pct:.1f}% of all, {pct_rated:.1f}% of rated)\")\n",
    "\n",
    "# Cross-analysis: difficulty by trail type\n",
    "if \"difficulty\" in hiking_attrs.columns and \"trail_type\" in hiking_attrs.columns:\n",
    "    print(\"\\n4. DIFFICULTY BY TRAIL TYPE\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Get trails with both difficulty and type\n",
    "    both_fields = hiking_attrs[hiking_attrs[\"difficulty\"].notna() & hiking_attrs[\"trail_type\"].notna()]\n",
    "\n",
    "    if len(both_fields) > 0:\n",
    "        print(f\"Trails with both difficulty and type: {len(both_fields):,}\")\n",
    "\n",
    "        # Create crosstab\n",
    "        cross_tab = pd.crosstab(both_fields[\"trail_type\"], both_fields[\"difficulty\"])\n",
    "\n",
    "        print(\"\\nDifficulty distribution by trail type:\")\n",
    "        for trail_type in cross_tab.index[:5]:  # Show top 5 trail types\n",
    "            print(f\"\\n{trail_type}:\")\n",
    "            row_total = cross_tab.loc[trail_type].sum()\n",
    "            for difficulty in cross_tab.columns:\n",
    "                count = cross_tab.loc[trail_type, difficulty]\n",
    "                if count > 0:\n",
    "                    pct = count / row_total * 100\n",
    "                    print(f\"  {difficulty}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic visualizations for main trail layer\n",
    "hiking_trail_layer = \"hiking_trail_centerline\" if trail_data.language == Language.EN else \"fotrute_senterlinje\"\n",
    "\n",
    "if hiking_trail_layer in trail_data.spatial_layers:\n",
    "    trails = trail_data.spatial_layers[hiking_trail_layer]\n",
    "\n",
    "    # Plot all trails\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    # Plot trails with a simple color\n",
    "    trails.plot(ax=ax, linewidth=0.5, alpha=0.6, color=\"darkgreen\")\n",
    "\n",
    "    ax.set_title(f\"Norwegian Hiking Trails ({len(trails)} features)\", fontsize=14)\n",
    "    ax.set_xlabel(\"X Coordinate\")\n",
    "    ax.set_ylabel(\"Y Coordinate\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add CRS info\n",
    "    ax.text(\n",
    "        0.02,\n",
    "        0.98,\n",
    "        f\"CRS: {trails.crs}\",\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=10,\n",
    "        verticalalignment=\"top\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DNT Trail Analysis\n",
    "\n",
    "Analyze trails maintained by DNT (Den Norske Turistforening - Norwegian Trekking Association) and its member organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify DNT-maintained trail segments and check for DNT origin vs maintenance discrepancies\n",
    "print(\"DNT TRAIL SEGMENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Ensure we have the required variables from earlier sections\n",
    "hiking_trail_layer = \"hiking_trail_centerline\" if trail_data.language == Language.EN else \"fotrute_senterlinje\"\n",
    "trails = trail_data.spatial_layers[hiking_trail_layer]\n",
    "hiking_attrs_layer = \"hiking_trail_info_table\" if trail_data.language == Language.EN else \"fotrute_info_tabell\"\n",
    "hiking_attrs = trail_data.attribute_tables[hiking_attrs_layer]\n",
    "\n",
    "# Filter attribute table for DNT-maintained trails\n",
    "# DNT can appear as \"DNT\" or as part of organization name (e.g., \"DNT Oslo og omegn\", \"Turistforening\")\n",
    "dnt_attrs = hiking_attrs[hiking_attrs[\"maintenance_responsible\"].str.contains(\"DNT|Turistforening\", case=False, na=False)]\n",
    "\n",
    "print(\"\\n1. DNT TRAIL RECORDS IN ATTRIBUTE TABLE\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total DNT-maintained trail records: {len(dnt_attrs):,}\")\n",
    "print(f\"Percentage of all trails: {len(dnt_attrs) / len(hiking_attrs) * 100:.1f}%\")\n",
    "\n",
    "# Get unique segment IDs (foreign keys to spatial layer)\n",
    "unique_dnt_segment_ids = dnt_attrs[\"hiking_trail_fk\"].unique()\n",
    "print(f\"\\nUnique DNT-maintained segments: {len(unique_dnt_segment_ids):,}\")\n",
    "print(f\"Percentage of all unique segments: {len(unique_dnt_segment_ids) / len(trails) * 100:.1f}%\")\n",
    "\n",
    "# Get the unique segments from spatial layer\n",
    "dnt_segments = trails[trails[\"local_id\"].isin(unique_dnt_segment_ids)]\n",
    "\n",
    "print(\"\\n2. DNT SEGMENT LENGTH ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"DNT segments found in spatial layer: {len(dnt_segments):,}\")\n",
    "\n",
    "# Calculate total length of DNT segments\n",
    "total_dnt_length = dnt_segments[\"SHAPE_Length\"].sum()\n",
    "print(f\"Total length of DNT segments: {total_dnt_length / 1000:,.1f} km\")\n",
    "print(f\"Percentage of total trail length: {total_dnt_length / trails['SHAPE_Length'].sum() * 100:.1f}%\")\n",
    "\n",
    "# Average segment length\n",
    "avg_dnt_length = dnt_segments[\"SHAPE_Length\"].mean()\n",
    "median_dnt_length = dnt_segments[\"SHAPE_Length\"].median()\n",
    "print(f\"\\nAverage DNT segment length: {avg_dnt_length / 1000:.2f} km\")\n",
    "print(f\"Median DNT segment length: {median_dnt_length / 1000:.2f} km\")\n",
    "\n",
    "# DNT organization breakdown\n",
    "print(\"\\n3. DNT ORGANIZATIONS\")\n",
    "print(\"-\" * 50)\n",
    "dnt_org_counts = dnt_attrs[\"maintenance_responsible\"].value_counts()\n",
    "print(f\"Unique DNT organizations: {dnt_org_counts.nunique()}\")\n",
    "\n",
    "print(\"\\nTop 10 DNT organizations:\")\n",
    "for org, count in dnt_org_counts.head(10).items():\n",
    "    pct = count / len(dnt_attrs) * 100\n",
    "    print(f\"  {org}: {count:,} records ({pct:.1f}%)\")\n",
    "\n",
    "# CHECK FOR DNT ORIGIN VS DNT MAINTENANCE DISCREPANCIES\n",
    "print(\"\\n4. ⚠️ DNT ORIGIN VS MAINTENANCE DISCREPANCY ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check for segments with DNT origin\n",
    "if \"origin\" in trails.columns:\n",
    "    dnt_origin_segments = trails[trails[\"origin\"].str.contains(\"DNT|Turistforening\", case=False, na=False)]\n",
    "    print(f\"Segments with DNT in origin field: {len(dnt_origin_segments):,}\")\n",
    "\n",
    "    # Find segments with DNT origin but NOT DNT maintained\n",
    "    dnt_origin_ids = set(dnt_origin_segments[\"local_id\"])\n",
    "    dnt_maintained_ids = set(unique_dnt_segment_ids)\n",
    "\n",
    "    dnt_origin_not_maintained = dnt_origin_ids - dnt_maintained_ids\n",
    "    dnt_maintained_not_origin = dnt_maintained_ids - dnt_origin_ids\n",
    "    both_dnt = dnt_origin_ids & dnt_maintained_ids\n",
    "\n",
    "    print(\"\\nDiscrepancy Breakdown:\")\n",
    "    print(f\"  ✓ DNT origin AND DNT maintained: {len(both_dnt):,} segments\")\n",
    "    print(f\"  ⚠️ DNT origin but NOT DNT maintained: {len(dnt_origin_not_maintained):,} segments\")\n",
    "    print(f\"  📊 DNT maintained but NOT DNT origin: {len(dnt_maintained_not_origin):,} segments\")\n",
    "\n",
    "    if len(dnt_origin_not_maintained) > 0:\n",
    "        print(f\"\\n⚠️ DISCREPANCY FOUND: {len(dnt_origin_not_maintained):,} segments with DNT origin are maintained by others!\")\n",
    "\n",
    "        # Get details about these segments\n",
    "        discrepancy_segments = trails[trails[\"local_id\"].isin(dnt_origin_not_maintained)]\n",
    "\n",
    "        # Find who maintains these DNT-origin segments\n",
    "        discrepancy_attrs = hiking_attrs[hiking_attrs[\"hiking_trail_fk\"].isin(dnt_origin_not_maintained)]\n",
    "\n",
    "        if len(discrepancy_attrs) > 0:\n",
    "            print(\"\\nWho maintains the DNT-origin segments:\")\n",
    "            maint_counts = discrepancy_attrs[\"maintenance_responsible\"].value_counts()\n",
    "            for maint, count in maint_counts.items():\n",
    "                print(f\"  • {maint}: {count:,} records\")\n",
    "\n",
    "            # Calculate total length of these discrepancy segments\n",
    "            discrepancy_length = discrepancy_segments[\"SHAPE_Length\"].sum()\n",
    "            print(f\"\\nTotal length of DNT-origin segments maintained by others: {discrepancy_length / 1000:,.1f} km\")\n",
    "\n",
    "            # Show some details about these segments\n",
    "            print(\"\\nDetails of discrepancy segments:\")\n",
    "            print(f\"  Average length: {discrepancy_segments['SHAPE_Length'].mean() / 1000:.2f} km\")\n",
    "            print(f\"  Total segments: {len(discrepancy_segments)}\")\n",
    "        else:\n",
    "            print(\"    (No maintenance info found for these segments)\")\n",
    "else:\n",
    "    print(\"  'origin' column not found in spatial layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create joined GeoDataFrame with DNT trails (includes duplicates for segments in multiple trails)\n",
    "print(\"CREATING JOINED DNT GEODATAFRAME\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Join spatial data with attribute data\n",
    "# This will create duplicate geometries for segments that are part of multiple trails\n",
    "dnt_joined_gdf = dnt_segments.merge(dnt_attrs, left_on=\"local_id\", right_on=\"hiking_trail_fk\", how=\"inner\")\n",
    "\n",
    "print(\"\\n1. JOINED DATA STATISTICS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total rows in joined GeoDataFrame: {len(dnt_joined_gdf):,}\")\n",
    "print(f\"Unique segments: {dnt_joined_gdf['local_id'].nunique():,}\")\n",
    "print(f\"Duplicate factor: {len(dnt_joined_gdf) / dnt_joined_gdf['local_id'].nunique():.2f}x\")\n",
    "\n",
    "# Trail name distribution in DNT trails\n",
    "print(\"\\n2. DNT TRAIL NAMES\")\n",
    "print(\"-\" * 50)\n",
    "named_dnt = dnt_joined_gdf[\"trail_name\"].notna().sum()\n",
    "print(f\"Named DNT trail records: {named_dnt:,} ({named_dnt / len(dnt_joined_gdf) * 100:.1f}%)\")\n",
    "print(f\"Unique trail names: {dnt_joined_gdf['trail_name'].nunique():,}\")\n",
    "\n",
    "print(\"\\nTop 10 DNT trail names:\")\n",
    "dnt_trail_names = dnt_joined_gdf[\"trail_name\"].value_counts()\n",
    "for name, count in dnt_trail_names.head(10).items():  # type: ignore[assignment]\n",
    "    if pd.notna(name):\n",
    "        pct = count / len(dnt_joined_gdf) * 100\n",
    "        print(f\"  {name}: {count:,} records ({pct:.1f}%)\")\n",
    "\n",
    "# Difficulty distribution\n",
    "print(\"\\n3. DNT TRAIL DIFFICULTY\")\n",
    "print(\"-\" * 50)\n",
    "if \"difficulty\" in dnt_joined_gdf.columns:\n",
    "    diff_counts = dnt_joined_gdf[\"difficulty\"].value_counts()\n",
    "    diff_total = dnt_joined_gdf[\"difficulty\"].notna().sum()\n",
    "\n",
    "    print(f\"DNT trails with difficulty rating: {diff_total:,} ({diff_total / len(dnt_joined_gdf) * 100:.1f}%)\")\n",
    "\n",
    "    print(\"\\nDifficulty distribution:\")\n",
    "    for difficulty, count in diff_counts.items():\n",
    "        pct = count / len(dnt_joined_gdf) * 100\n",
    "        print(f\"  {difficulty}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Store for later use\n",
    "print(f\"\\n✓ Created joined GeoDataFrame 'dnt_joined_gdf' with {len(dnt_joined_gdf):,} records\")\n",
    "print(f\"✓ Created unique segments GeoDataFrame 'dnt_segments' with {len(dnt_segments):,} segments\")\n",
    "\n",
    "dnt_joined_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DNT trails on empty canvas\n",
    "print(\"VISUALIZING DNT TRAILS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Plot 1: All trails with DNT highlighted\n",
    "print(\"\\n1. DNT trails highlighted among all trails\")\n",
    "ax1.set_title(\"DNT-Maintained Trails (Highlighted in Red)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Plot all trails in light gray as background\n",
    "trails.plot(ax=ax1, linewidth=0.3, alpha=0.3, color=\"lightgray\", label=\"Other trails\")\n",
    "\n",
    "# Overlay DNT segments in red\n",
    "dnt_segments.plot(ax=ax1, linewidth=0.8, alpha=0.8, color=\"red\", label=\"DNT trails\")\n",
    "\n",
    "ax1.set_xlabel(\"X Coordinate (EPSG:25833)\")\n",
    "ax1.set_ylabel(\"Y Coordinate (EPSG:25833)\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(loc=\"upper right\")\n",
    "\n",
    "# Add statistics text\n",
    "stats_text = (\n",
    "    f\"DNT Segments: {len(dnt_segments):,}\\n\"\n",
    "    f\"Total Length: {total_dnt_length / 1000:,.0f} km\\n\"\n",
    "    f\"({len(dnt_segments) / len(trails) * 100:.1f}% of all segments)\"\n",
    ")\n",
    "ax1.text(\n",
    "    0.02,\n",
    "    0.98,\n",
    "    stats_text,\n",
    "    transform=ax1.transAxes,\n",
    "    fontsize=10,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox={\"boxstyle\": \"round\", \"facecolor\": \"white\", \"alpha\": 0.8},\n",
    ")\n",
    "\n",
    "# Plot 2: Only DNT trails\n",
    "ax2.set_title(\"DNT-Maintained Trails Only\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Plot only DNT segments\n",
    "dnt_segments.plot(ax=ax2, linewidth=0.6, alpha=0.7, color=\"darkblue\")\n",
    "\n",
    "ax2.set_xlabel(\"X Coordinate (EPSG:25833)\")\n",
    "ax2.set_ylabel(\"Y Coordinate (EPSG:25833)\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add CRS info\n",
    "ax2.text(0.02, 0.98, f\"CRS: {dnt_segments.crs}\", transform=ax2.transAxes, fontsize=10, verticalalignment=\"top\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Visualized {len(dnt_segments):,} unique DNT segments\")\n",
    "print(f\"✓ Total length: {total_dnt_length / 1000:,.1f} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive DNT trail summary\n",
    "print(\"DNT TRAIL ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n📊 KEY METRICS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Unique DNT-maintained segments: {len(dnt_segments):,}\")\n",
    "print(f\"Total DNT records (with duplicates): {len(dnt_joined_gdf):,}\")\n",
    "print(f\"Total length (unique segments): {total_dnt_length / 1000:,.1f} km\")\n",
    "print(f\"Coverage: {len(dnt_segments) / len(trails) * 100:.1f}% of all trail segments\")\n",
    "\n",
    "print(\"\\n🏢 ORGANIZATION BREAKDOWN\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total DNT organizations: {dnt_org_counts.nunique()}\")\n",
    "\n",
    "# Group by main DNT vs local associations\n",
    "main_dnt = dnt_attrs[dnt_attrs[\"maintenance_responsible\"] == \"DNT\"]\n",
    "local_dnt = dnt_attrs[dnt_attrs[\"maintenance_responsible\"] != \"DNT\"]\n",
    "\n",
    "print(f\"\\nMain DNT organization: {len(main_dnt):,} records\")\n",
    "print(f\"Local DNT associations: {len(local_dnt):,} records\")\n",
    "\n",
    "# Top local associations\n",
    "print(\"\\nTop 5 local DNT associations:\")\n",
    "local_org_counts = local_dnt[\"maintenance_responsible\"].value_counts()\n",
    "for org, count in local_org_counts.head(5).items():\n",
    "    print(f\"  • {org}: {count:,} records\")\n",
    "\n",
    "print(\"\\n📏 LENGTH STATISTICS\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Average segment length: {avg_dnt_length / 1000:.2f} km\")\n",
    "print(f\"Median segment length: {median_dnt_length / 1000:.2f} km\")\n",
    "print(f\"Shortest segment: {dnt_segments['SHAPE_Length'].min():.1f} m\")\n",
    "print(f\"Longest segment: {dnt_segments['SHAPE_Length'].max() / 1000:.1f} km\")\n",
    "\n",
    "# Length distribution\n",
    "length_bins = [0, 100, 500, 1000, 5000, 10000, float(\"inf\")]\n",
    "length_labels = [\"<100m\", \"100-500m\", \"500m-1km\", \"1-5km\", \"5-10km\", \">10km\"]\n",
    "dnt_segments[\"length_bin\"] = pd.cut(dnt_segments[\"SHAPE_Length\"], bins=length_bins, labels=length_labels)\n",
    "\n",
    "print(\"\\nLength distribution:\")\n",
    "for bin_label in length_labels:\n",
    "    count = (dnt_segments[\"length_bin\"] == bin_label).sum()\n",
    "    pct = count / len(dnt_segments) * 100\n",
    "    print(f\"  {bin_label}: {count:,} segments ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n🏔️ TRAIL CHARACTERISTICS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Marking information\n",
    "if \"marking\" in dnt_segments.columns:\n",
    "    marking_counts = dnt_segments[\"marking\"].value_counts()\n",
    "    print(\"\\nMarking status:\")\n",
    "    for marking, count in marking_counts.items():\n",
    "        pct = count / len(dnt_segments) * 100\n",
    "        print(f\"  {marking}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Trail difficulty from joined data\n",
    "if \"difficulty\" in dnt_joined_gdf.columns:\n",
    "    print(\"\\nDifficulty ratings (from all DNT records):\")\n",
    "    diff_with_rating = dnt_joined_gdf[\"difficulty\"].notna().sum()\n",
    "    print(f\"  Records with difficulty: {diff_with_rating:,} ({diff_with_rating / len(dnt_joined_gdf) * 100:.1f}%)\")\n",
    "\n",
    "# Trail significance\n",
    "if \"trail_significance\" in dnt_joined_gdf.columns:\n",
    "    sig_counts = dnt_joined_gdf[\"trail_significance\"].value_counts()\n",
    "    if len(sig_counts) > 0:\n",
    "        print(\"\\nTrail significance:\")\n",
    "        for sig, count in sig_counts.items():\n",
    "            pct = count / len(dnt_joined_gdf) * 100\n",
    "            print(f\"  {sig}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✓ DNT TRAIL ANALYSIS COMPLETE\")\n",
    "print(\"✓ Data available in: 'dnt_segments' (unique) and 'dnt_joined_gdf' (with duplicates)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
