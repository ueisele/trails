{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norwegian Trails Data Exploration\n",
    "\n",
    "**Goal**: Understand the actual structure and content of Turrutebasen data from Geonorge before building abstractions.\n",
    "\n",
    "**Data Source**: Geonorge/Kartverket - Norwegian government's official trail database (Turrutebasen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Our minimal modules\n",
    "from trails.io.sources.geonorge import Source as GeonorgeSource\n",
    "from trails.io.sources.language import Language\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Project paths\n",
    "project_root = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "cache_dir = project_root / \".cache\"\n",
    "\n",
    "# Display settings\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Cache directory: {cache_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Geonorge Data\n",
    "\n",
    "First, let's load the data and see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Geonorge source and load data\n",
    "geonorge = GeonorgeSource(cache_dir=str(cache_dir))\n",
    "\n",
    "# Load with English translations (use Language.NO for Norwegian)\n",
    "trail_data = geonorge.load_turrutebasen(language=Language.EN)\n",
    "print(\"Data Source Information:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: {trail_data.metadata.dataset_name}\")\n",
    "print(f\"Provider: {trail_data.metadata.provider}\")\n",
    "print(f\"License: {trail_data.metadata.license}\")\n",
    "print(f\"Dataset ID: {trail_data.metadata.dataset_id}\")\n",
    "print(f\"Description: {trail_data.metadata.description}\")\n",
    "print(f\"Attribution: {trail_data.metadata.attribution}\")\n",
    "\n",
    "print(f\"\\nData loaded successfully from: {trail_data.source_url}\")\n",
    "print(f\"Version: {trail_data.version}\")\n",
    "print(f\"Language: {trail_data.language.value}\")\n",
    "print(f\"CRS: {trail_data.crs}\")\n",
    "print(f\"Total features: {trail_data.total_features}\")\n",
    "\n",
    "# Layers and columns are now automatically translated\n",
    "print(f\"\\nFound {len(trail_data.spatial_layers)} spatial layers:\")\n",
    "for layer_name, gdf in trail_data.spatial_layers.items():\n",
    "    print(f\"  - {layer_name}: {len(gdf)} features\")\n",
    "\n",
    "print(f\"\\nFound {len(trail_data.attribute_tables)} attribute tables:\")\n",
    "for table_name, df in trail_data.attribute_tables.items():\n",
    "    print(f\"  - {table_name}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complete Layer Analysis\n",
    "\n",
    "Let's examine each layer in detail, analyzing both structure and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete analysis of all layers\n",
    "print(\"=\" * 80)\n",
    "print(\"SPATIAL LAYERS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "layer_summary = {}\n",
    "\n",
    "for layer_name, gdf in trail_data.spatial_layers.items():\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"LAYER: {layer_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Basic info\n",
    "    print(\"\\nBasic Information:\")\n",
    "    print(f\"  Shape: {gdf.shape}\")\n",
    "    print(f\"  CRS: {gdf.crs}\")\n",
    "\n",
    "    # Geometry info\n",
    "    print(\"\\nGeometry Information:\")\n",
    "    if not gdf.empty and \"geometry\" in gdf.columns:\n",
    "        geom_types = gdf.geometry.geom_type.unique()\n",
    "        print(f\"  Geometry types: {geom_types}\")\n",
    "        print(f\"  Total bounds: {gdf.total_bounds}\")\n",
    "\n",
    "    # Column analysis\n",
    "    columns = [col for col in gdf.columns if col != \"geometry\"]\n",
    "    print(f\"\\nColumn Analysis ({len(columns)} columns):\")\n",
    "\n",
    "    for col in columns:\n",
    "        dtype = gdf[col].dtype\n",
    "        non_null_count = gdf[col].notna().sum()\n",
    "        null_pct = (1 - non_null_count / len(gdf)) * 100\n",
    "\n",
    "        print(f\"\\n  {col}:\")\n",
    "        print(f\"    Type: {dtype}\")\n",
    "        print(f\"    Non-null: {non_null_count}/{len(gdf)} ({100 - null_pct:.1f}% complete)\")\n",
    "\n",
    "        # For categorical/text columns\n",
    "        if dtype == \"object\" or dtype == \"string\" and non_null_count > 0:\n",
    "            unique_count = gdf[col].nunique()\n",
    "            print(f\"    Unique values: {unique_count}\")\n",
    "\n",
    "            if unique_count <= 5:\n",
    "                values = gdf[col].dropna().unique()\n",
    "                # Values are already expanded and translated\n",
    "                print(f\"    Values: {list(values)}\")\n",
    "            elif unique_count > 5:\n",
    "                top_values = gdf[col].value_counts().head(5)\n",
    "                print(\"    Top 5 values:\")\n",
    "                for val, count in top_values.items():\n",
    "                    # Values are already expanded and translated\n",
    "                    print(f\"      - {val}: {count}\")\n",
    "\n",
    "        # For numeric columns\n",
    "        elif dtype in [\"Int64\", \"Float64\"] and non_null_count > 0:\n",
    "            print(f\"    Min: {gdf[col].min()}\")\n",
    "            print(f\"    Max: {gdf[col].max()}\")\n",
    "            print(f\"    Mean: {gdf[col].mean():.2f}\")\n",
    "\n",
    "        # For datetime columns\n",
    "        elif dtype in [\"datetime64[ms, UTC]\"] and non_null_count > 0:\n",
    "            print(f\"    Min: {gdf[col].min()}\")\n",
    "            print(f\"    Max: {gdf[col].max()}\")\n",
    "\n",
    "    # Store summary\n",
    "    layer_summary[layer_name] = {\n",
    "        \"shape\": gdf.shape,\n",
    "        \"crs\": str(gdf.crs),\n",
    "        \"columns\": list(gdf.columns),\n",
    "        \"geometry_types\": list(geom_types) if not gdf.empty else [],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ATTRIBUTE TABLES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for table_name, df in trail_data.attribute_tables.items():\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"TABLE: {table_name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Basic info\n",
    "    print(\"\\nBasic Information:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "\n",
    "    # Column analysis\n",
    "    print(f\"\\nColumn Analysis ({len(df.columns)} columns):\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        non_null_count = df[col].notna().sum()\n",
    "        null_pct = (1 - non_null_count / len(df)) * 100\n",
    "\n",
    "        print(f\"\\n  {col}:\")\n",
    "        print(f\"    Type: {dtype}\")\n",
    "        print(f\"    Non-null: {non_null_count}/{len(df)} ({100 - null_pct:.1f}% complete)\")\n",
    "\n",
    "        # For categorical/text columns\n",
    "        if dtype == \"object\" or dtype == \"string\" and non_null_count > 0:\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"    Unique values: {unique_count}\")\n",
    "\n",
    "            if unique_count <= 5:\n",
    "                values = df[col].dropna().unique()\n",
    "                # Values are already expanded and translated\n",
    "                print(f\"    Values: {list(values)}\")\n",
    "            elif unique_count > 5:\n",
    "                top_values = df[col].value_counts().head(5)\n",
    "                print(\"    Top 5 values:\")\n",
    "                for val, count in top_values.items():\n",
    "                    # Values are already expanded and translated\n",
    "                    print(f\"      - {val}: {count}\")\n",
    "\n",
    "        # For numeric columns\n",
    "        elif dtype in [\"Int64\", \"Float64\"] and non_null_count > 0:\n",
    "            print(f\"    Min: {df[col].min()}\")\n",
    "            print(f\"    Max: {df[col].max()}\")\n",
    "            mean_val = df[col].mean()\n",
    "            if not pd.isna(mean_val):\n",
    "                print(f\"    Mean: {mean_val:.2f}\")\n",
    "\n",
    "        # For datetime columns\n",
    "        elif dtype in [\"datetime64[ms, UTC]\"] and non_null_count > 0:\n",
    "            print(f\"    Min: {df[col].min()}\")\n",
    "            print(f\"    Max: {df[col].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify Trail Layers\n",
    "\n",
    "Let's identify which layers contain actual trail data (LineString geometries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find trail layers (with line geometries)\n",
    "trail_layers = {}\n",
    "point_layers = {}\n",
    "other_layers = {}\n",
    "\n",
    "for layer_name, gdf in trail_data.spatial_layers.items():\n",
    "    if gdf.empty:\n",
    "        other_layers[layer_name] = gdf\n",
    "        continue\n",
    "\n",
    "    # Check geometry type\n",
    "    geom_types = gdf.geometry.geom_type.unique()\n",
    "\n",
    "    if any(gt in [\"LineString\", \"MultiLineString\"] for gt in geom_types):\n",
    "        trail_layers[layer_name] = gdf\n",
    "    elif any(gt in [\"Point\", \"MultiPoint\"] for gt in geom_types):\n",
    "        point_layers[layer_name] = gdf\n",
    "    else:\n",
    "        other_layers[layer_name] = gdf\n",
    "\n",
    "print(\"Layer Classification:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTrail Layers (Lines): {len(trail_layers)}\")\n",
    "for name, gdf in trail_layers.items():\n",
    "    print(f\"  - {name}: {len(gdf)} features\")\n",
    "\n",
    "print(f\"\\nPoint Layers (Facilities): {len(point_layers)}\")\n",
    "for name, gdf in point_layers.items():\n",
    "    print(f\"  - {name}: {len(gdf)} features\")\n",
    "\n",
    "if other_layers:\n",
    "    print(f\"\\nOther Layers: {len(other_layers)}\")\n",
    "    for name, gdf in other_layers.items():\n",
    "        print(f\"  - {name}: {len(gdf)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Structure & Relationships\n",
    "\n",
    "Let's understand how the hiking trail data is organized in the database, including spatial layers, attribute tables, and their relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Spatial Layer Schema\n",
    "\n",
    "Analyze the structure and fields of the spatial layer containing trail geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze spatial layer schema for hiking trails\n",
    "hiking_trail_layer = \"hiking_trail_centerline\" if trail_data.language == Language.EN else \"fotrute_senterlinje\"\n",
    "\n",
    "if hiking_trail_layer in trail_data.spatial_layers:\n",
    "    trails = trail_data.spatial_layers[hiking_trail_layer]\n",
    "\n",
    "    print(\"SPATIAL LAYER SCHEMA ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Basic schema info\n",
    "    print(\"\\n1. LAYER STRUCTURE\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Layer name: {hiking_trail_layer}\")\n",
    "    print(f\"Total features: {len(trails):,}\")\n",
    "    print(f\"CRS: {trails.crs}\")\n",
    "    print(f\"Geometry type: {trails.geometry.geom_type.unique()}\")\n",
    "    print(f\"Total columns: {len(trails.columns)}\")\n",
    "\n",
    "    # Analyze column types\n",
    "    print(\"\\n2. COLUMN TYPES\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Group columns by data type\n",
    "    spatial_type_groups: dict[str, list[str]] = {}\n",
    "    for col in trails.columns:\n",
    "        if col != \"geometry\":\n",
    "            dtype_str = str(trails[col].dtype)\n",
    "            if dtype_str not in spatial_type_groups:\n",
    "                spatial_type_groups[dtype_str] = []\n",
    "            spatial_type_groups[dtype_str].append(col)\n",
    "\n",
    "    for dtype_str, cols in sorted(spatial_type_groups.items()):\n",
    "        print(f\"\\n{dtype_str} columns ({len(cols)}):\")\n",
    "        for col in sorted(cols):\n",
    "            print(f\"  - {col}\")\n",
    "\n",
    "    # Identify key fields with types\n",
    "    print(\"\\n3. KEY IDENTIFIER FIELDS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    id_fields = [\"local_id\", \"version_id\", \"namespace\", \"area_id\"]\n",
    "    for field in id_fields:\n",
    "        if field in trails.columns:\n",
    "            dtype = trails[field].dtype\n",
    "            unique_count = trails[field].nunique()\n",
    "            print(f\"{field} [{dtype}]:\")\n",
    "            print(f\"  Unique values: {unique_count:,}\")\n",
    "            print(f\"  Is unique: {unique_count == len(trails)}\")\n",
    "            if field == \"local_id\":\n",
    "                print(f\"  Sample: {trails[field].iloc[0]}\")\n",
    "\n",
    "    # Metadata fields with types\n",
    "    print(\"\\n4. METADATA FIELDS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    metadata_fields = [\"data_capture_date\", \"update_date\", \"copy_date\", \"accuracy\", \"original_data_host\"]\n",
    "    for field in metadata_fields:\n",
    "        if field in trails.columns:\n",
    "            dtype = trails[field].dtype\n",
    "            non_null = trails[field].notna().sum()\n",
    "            completeness = (non_null / len(trails)) * 100\n",
    "            print(f\"{field} [{dtype}]: {completeness:.1f}% complete\")\n",
    "\n",
    "    # Trail attribute fields (minimal in spatial layer) with types\n",
    "    print(\"\\n5. TRAIL ATTRIBUTE FIELDS (in spatial layer)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    attr_fields = [\"object_type\", \"signage\", \"lighting\", \"marking\", \"season\", \"trail_width\", \"surface_type\", \"traffic_load\"]\n",
    "\n",
    "    for field in attr_fields:\n",
    "        if field in trails.columns:\n",
    "            dtype = trails[field].dtype\n",
    "            non_null = trails[field].notna().sum()\n",
    "            completeness = (non_null / len(trails)) * 100\n",
    "            unique_vals = trails[field].nunique()\n",
    "            print(f\"{field} [{dtype}]:\")\n",
    "            print(f\"  Completeness: {completeness:.1f}%\")\n",
    "            print(f\"  Unique values: {unique_vals}\")\n",
    "            if completeness > 0 and unique_vals <= 5:\n",
    "                print(f\"  Values: {list(trails[field].dropna().unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Attribute Table Schema\n",
    "\n",
    "Analyze the structure of the attribute table that contains detailed trail information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attribute table schema\n",
    "hiking_attrs_layer = \"hiking_trail_info_table\" if trail_data.language == Language.EN else \"fotrute_info_tabell\"\n",
    "\n",
    "if hiking_attrs_layer in trail_data.attribute_tables:\n",
    "    hiking_attrs = trail_data.attribute_tables[hiking_attrs_layer]\n",
    "\n",
    "    print(\"ATTRIBUTE TABLE SCHEMA ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Basic schema info\n",
    "    print(\"\\n1. TABLE STRUCTURE\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Table name: {hiking_attrs_layer}\")\n",
    "    print(f\"Total rows: {len(hiking_attrs):,}\")\n",
    "    print(f\"Total columns: {len(hiking_attrs.columns)}\")\n",
    "\n",
    "    # Column types\n",
    "    print(\"\\n2. COLUMN TYPES\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    attr_type_groups: dict[str, list[str]] = {}\n",
    "    for col in hiking_attrs.columns:\n",
    "        dtype_str = str(hiking_attrs[col].dtype)\n",
    "        if dtype_str not in attr_type_groups:\n",
    "            attr_type_groups[dtype_str] = []\n",
    "        attr_type_groups[dtype_str].append(col)\n",
    "\n",
    "    for dtype_str, cols in sorted(attr_type_groups.items()):\n",
    "        print(f\"\\n{dtype_str} columns ({len(cols)}):\")\n",
    "        for col in sorted(cols):\n",
    "            print(f\"  - {col}\")\n",
    "\n",
    "    # Key fields with types\n",
    "    print(\"\\n3. KEY FIELDS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Primary key (if exists)\n",
    "    if \"object_id\" in hiking_attrs.columns:\n",
    "        dtype = hiking_attrs[\"object_id\"].dtype\n",
    "        print(f\"object_id [{dtype}] (Primary Key):\")\n",
    "        print(f\"  Unique values: {hiking_attrs['object_id'].nunique():,}\")\n",
    "        print(f\"  Is unique: {hiking_attrs['object_id'].nunique() == len(hiking_attrs)}\")\n",
    "\n",
    "    # Foreign key\n",
    "    if \"hiking_trail_fk\" in hiking_attrs.columns:\n",
    "        dtype = hiking_attrs[\"hiking_trail_fk\"].dtype\n",
    "        print(f\"\\nhiking_trail_fk [{dtype}] (Foreign Key to spatial layer):\")\n",
    "        print(f\"  Unique values: {hiking_attrs['hiking_trail_fk'].nunique():,}\")\n",
    "        print(f\"  Total references: {len(hiking_attrs):,}\")\n",
    "        print(f\"  Average rows per FK: {len(hiking_attrs) / hiking_attrs['hiking_trail_fk'].nunique():.2f}\")\n",
    "\n",
    "    # Information fields with types\n",
    "    print(\"\\n4. INFORMATION FIELDS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    info_fields = [\n",
    "        \"trail_name\",\n",
    "        \"trail_number\",\n",
    "        \"trail_type\",\n",
    "        \"special_hiking_trail_type\",\n",
    "        \"accessibility\",\n",
    "        \"difficulty\",\n",
    "        \"trail_significance\",\n",
    "        \"trail_information\",\n",
    "    ]\n",
    "\n",
    "    for field in info_fields:\n",
    "        if field in hiking_attrs.columns:\n",
    "            dtype = hiking_attrs[field].dtype\n",
    "            non_null = hiking_attrs[field].notna().sum()\n",
    "            completeness = (non_null / len(hiking_attrs)) * 100\n",
    "            unique_vals = hiking_attrs[field].nunique()\n",
    "            print(f\"\\n{field} [{dtype}]:\")\n",
    "            print(f\"  Completeness: {completeness:.1f}%\")\n",
    "            print(f\"  Unique values: {unique_vals}\")\n",
    "\n",
    "    # Administrative fields with types\n",
    "    print(\"\\n5. ADMINISTRATIVE FIELDS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    admin_fields = [\"maintenance_responsible\"]\n",
    "\n",
    "    for field in admin_fields:\n",
    "        if field in hiking_attrs.columns:\n",
    "            dtype = hiking_attrs[field].dtype\n",
    "            non_null = hiking_attrs[field].notna().sum()\n",
    "            completeness = (non_null / len(hiking_attrs)) * 100\n",
    "            unique_vals = hiking_attrs[field].nunique()\n",
    "            print(f\"\\n{field} [{dtype}]:\")\n",
    "            print(f\"  Completeness: {completeness:.1f}%\")\n",
    "            print(f\"  Unique values: {unique_vals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Foreign Key Relationships\n",
    "\n",
    "Analyze how the spatial layer and attribute table are connected through foreign keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze foreign key relationships\n",
    "if \"hiking_trail_fk\" in hiking_attrs.columns:\n",
    "    print(\"FOREIGN KEY RELATIONSHIP ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    hiking_spatial = trail_data.spatial_layers[hiking_trail_layer]\n",
    "\n",
    "    # Basic relationship stats\n",
    "    print(\"\\n1. RELATIONSHIP OVERVIEW\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Spatial layer records: {len(hiking_spatial):,}\")\n",
    "    print(f\"Attribute table records: {len(hiking_attrs):,}\")\n",
    "    print(\"Relationship type: Many-to-One (many attribute rows -> one geometry)\")\n",
    "\n",
    "    # Analyze FK distribution\n",
    "    fk_counts = hiking_attrs[\"hiking_trail_fk\"].value_counts()\n",
    "\n",
    "    print(\"\\n2. FOREIGN KEY DISTRIBUTION\")\n",
    "    print(\"-\" * 50)\n",
    "    fk_dtype = hiking_attrs[\"hiking_trail_fk\"].dtype\n",
    "    print(f\"Foreign key column type: hiking_trail_fk [{fk_dtype}]\")\n",
    "    print(f\"Unique foreign keys: {hiking_attrs['hiking_trail_fk'].nunique():,}\")\n",
    "    print(f\"Should match spatial records: {len(hiking_spatial):,}\")\n",
    "    print(f\"Match: {'✓ Yes' if hiking_attrs['hiking_trail_fk'].nunique() == len(hiking_spatial) else '✗ No'}\")\n",
    "\n",
    "    print(\"\\nDistribution of attribute rows per geometry:\")\n",
    "    print(f\"  Min: {fk_counts.min()} row(s)\")\n",
    "    print(f\"  Max: {fk_counts.max()} row(s)\")\n",
    "    print(f\"  Mean: {fk_counts.mean():.2f} row(s)\")\n",
    "    print(f\"  Median: {fk_counts.median():.0f} row(s)\")\n",
    "\n",
    "    # Detailed distribution\n",
    "    print(\"\\n3. RELATIONSHIP CARDINALITY\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    cardinality_dist = fk_counts.value_counts().sort_index()\n",
    "\n",
    "    # Group into categories\n",
    "    one_to_one = (fk_counts == 1).sum()\n",
    "    one_to_two = (fk_counts == 2).sum()\n",
    "    one_to_many = (fk_counts > 2).sum()\n",
    "\n",
    "    print(f\"One-to-One (1 attribute row): {one_to_one:,} geometries ({one_to_one / len(fk_counts) * 100:.1f}%)\")\n",
    "    print(f\"One-to-Two (2 attribute rows): {one_to_two:,} geometries ({one_to_two / len(fk_counts) * 100:.1f}%)\")\n",
    "    print(f\"One-to-Many (3+ attribute rows): {one_to_many:,} geometries ({one_to_many / len(fk_counts) * 100:.1f}%)\")\n",
    "\n",
    "    # Show distribution for first 20 cardinalities\n",
    "    print(\"\\nDetailed cardinality distribution:\")\n",
    "    for n_rows, count in cardinality_dist.head(20).items():\n",
    "        pct = count / len(fk_counts) * 100\n",
    "        print(f\"  {n_rows} attribute row(s): {count:,} geometries ({pct:.1f}%)\")\n",
    "\n",
    "    # Analyze why multiple rows exist\n",
    "    print(\"\\n4. MULTIPLE ROWS ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Get FKs with multiple rows\n",
    "    multi_row_fks = fk_counts[fk_counts > 1].index\n",
    "\n",
    "    if len(multi_row_fks) > 0:\n",
    "        # Sample a FK with multiple rows\n",
    "        sample_fk = multi_row_fks[0]\n",
    "        sample_rows = hiking_attrs[hiking_attrs[\"hiking_trail_fk\"] == sample_fk]\n",
    "\n",
    "        print(f\"Example: FK '{sample_fk[:8]}...' has {len(sample_rows)} rows\")\n",
    "\n",
    "        # Check what differs between rows\n",
    "        varying_cols = []\n",
    "        for col in hiking_attrs.columns:\n",
    "            if col != \"hiking_trail_fk\" and sample_rows[col].nunique() > 1:\n",
    "                varying_cols.append(col)\n",
    "\n",
    "        print(f\"Columns that vary across these rows: {len(varying_cols)}\")\n",
    "        if \"trail_name\" in varying_cols:\n",
    "            name_dtype = hiking_attrs[\"trail_name\"].dtype\n",
    "            print(f\"\\nDifferent trail names [column type: {name_dtype}] for same segment:\")\n",
    "            for name in sample_rows[\"trail_name\"].unique()[:5]:\n",
    "                if pd.notna(name):\n",
    "                    print(f\"  - {name}\")\n",
    "\n",
    "    # Validation check\n",
    "    print(\"\\n5. DATA INTEGRITY CHECKS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Check if all FKs in attribute table exist in spatial layer\n",
    "    spatial_ids = set(hiking_spatial[\"local_id\"])\n",
    "    attr_fks = set(hiking_attrs[\"hiking_trail_fk\"])\n",
    "\n",
    "    # Get types for comparison\n",
    "    spatial_id_dtype = hiking_spatial[\"local_id\"].dtype\n",
    "    attr_fk_dtype = hiking_attrs[\"hiking_trail_fk\"].dtype\n",
    "\n",
    "    print(f\"Comparing: local_id [{spatial_id_dtype}] with hiking_trail_fk [{attr_fk_dtype}]\")\n",
    "\n",
    "    orphaned_fks = attr_fks - spatial_ids\n",
    "    print(f\"Orphaned foreign keys (in attributes but not spatial): {len(orphaned_fks)}\")\n",
    "\n",
    "    unreferenced_ids = spatial_ids - attr_fks\n",
    "    print(f\"Unreferenced geometries (in spatial but not attributes): {len(unreferenced_ids)}\")\n",
    "\n",
    "    if len(orphaned_fks) == 0 and len(unreferenced_ids) == 0:\n",
    "        print(\"✓ Perfect referential integrity - all relationships are valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Data Quality Assessment\n",
    "\n",
    "Evaluate the completeness and quality of the data across both layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Spatial layer quality\n",
    "print(\"\\n1. SPATIAL LAYER QUALITY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Geometry validation:\")\n",
    "print(f\"  Valid geometries: {trails.geometry.is_valid.sum():,} / {len(trails):,}\")\n",
    "print(f\"  Invalid geometries: {(~trails.geometry.is_valid).sum()}\")\n",
    "print(f\"  Empty geometries: {trails.geometry.is_empty.sum()}\")\n",
    "print(f\"  Simple geometries: {trails.geometry.is_simple.sum():,} / {len(trails):,}\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_geoms = trails.geometry.duplicated().sum()\n",
    "print(f\"  Duplicate geometries: {duplicate_geoms}\")\n",
    "\n",
    "# Field completeness in spatial layer\n",
    "print(\"\\nField completeness (spatial layer):\")\n",
    "spatial_completeness = []\n",
    "for col in trails.columns:\n",
    "    if col != \"geometry\":\n",
    "        dtype = trails[col].dtype\n",
    "        non_null = trails[col].notna().sum()\n",
    "        completeness = (non_null / len(trails)) * 100\n",
    "        spatial_completeness.append((col, dtype, completeness))\n",
    "\n",
    "# Sort by completeness\n",
    "spatial_completeness.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"  Fully complete fields (100%):\")\n",
    "for col, dtype, pct in spatial_completeness:\n",
    "    if pct == 100:\n",
    "        print(f\"    - {col} [{dtype}]\")\n",
    "\n",
    "print(\"  Partially populated fields (<100% & >=10%):\")\n",
    "partial_fields = [(col, dtype, pct) for col, dtype, pct in spatial_completeness if 100 > pct >= 10]\n",
    "for col, dtype, pct in partial_fields:\n",
    "    print(f\"    - {col} [{dtype}]: {pct:.1f}%\")\n",
    "\n",
    "print(\"  Poorly populated fields (<10%):\")\n",
    "poor_fields = [(col, dtype, pct) for col, dtype, pct in spatial_completeness if pct < 10]\n",
    "for col, dtype, pct in poor_fields:\n",
    "    print(f\"    - {col} [{dtype}]: {pct:.1f}%\")\n",
    "\n",
    "# Attribute table quality\n",
    "print(\"\\n2. ATTRIBUTE TABLE QUALITY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Critical fields completeness\n",
    "critical_fields = [\"trail_name\", \"difficulty\", \"trail_type\", \"trail_significance\", \"maintenance_responsible\", \"hiking_trail_fk\"]\n",
    "\n",
    "print(\"Critical field completeness:\")\n",
    "for field in critical_fields:\n",
    "    if field in hiking_attrs.columns:\n",
    "        dtype = hiking_attrs[field].dtype\n",
    "        non_null = hiking_attrs[field].notna().sum()\n",
    "        completeness = (non_null / len(hiking_attrs)) * 100\n",
    "        print(f\"  {field} [{dtype}]: {completeness:.1f}%\")\n",
    "\n",
    "# Check for unnamed trails\n",
    "if \"trail_name\" in hiking_attrs.columns:\n",
    "    name_dtype = hiking_attrs[\"trail_name\"].dtype\n",
    "    unnamed = hiking_attrs[\"trail_name\"].isna().sum()\n",
    "    unknown = (hiking_attrs[\"trail_name\"] == \"Ukjent\").sum() if \"Ukjent\" in hiking_attrs[\"trail_name\"].values else 0\n",
    "    total_unnamed = unnamed + unknown\n",
    "    print(f\"\\nUnnamed/unknown trails in trail_name [{name_dtype}]: {total_unnamed:,} ({total_unnamed / len(hiking_attrs) * 100:.1f}%)\")\n",
    "\n",
    "# Overall assessment\n",
    "print(\"\\n3. OVERALL DATA QUALITY SUMMARY\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate quality score\n",
    "quality_metrics = {\n",
    "    \"Geometry validity\": trails.geometry.is_valid.sum() / len(trails) * 100,\n",
    "    \"No duplicate geometries\": (len(trails) - duplicate_geoms) / len(trails) * 100,\n",
    "    \"Trail names present\": (len(hiking_attrs) - total_unnamed) / len(hiking_attrs) * 100 if \"trail_name\" in hiking_attrs.columns else 0,\n",
    "    \"Referential integrity\": 100 if len(orphaned_fks) == 0 and len(unreferenced_ids) == 0 else 0,\n",
    "}\n",
    "\n",
    "print(\"Quality metrics:\")\n",
    "for metric, score in quality_metrics.items():\n",
    "    status = \"✓\" if score >= 95 else \"⚠\" if score >= 80 else \"✗\"\n",
    "    print(f\"  {status} {metric}: {score:.1f}%\")\n",
    "\n",
    "avg_quality = sum(quality_metrics.values()) / len(quality_metrics)\n",
    "print(f\"\\nOverall quality score: {avg_quality:.1f}%\")\n",
    "\n",
    "# Data structure summary\n",
    "print(\"\\n4. DATA STRUCTURE INSIGHTS\")\n",
    "print(\"-\" * 50)\n",
    "print(\"✓ Normalized database design prevents geometry duplication\")\n",
    "print(\"✓ Many-to-one relationships allow multiple trail names per segment\")\n",
    "print(\"✓ Spatial layer contains minimal attributes (optimized for geometry)\")\n",
    "print(\"✓ Attribute table contains rich trail information\")\n",
    "print(f\"✓ Total unique trail segments: {len(trails):,}\")\n",
    "print(f\"✓ Total trail name assignments: {len(hiking_attrs):,}\")\n",
    "print(f\"✓ Segments with multiple names: {len(multi_row_fks):,} ({len(multi_row_fks) / len(trails) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Trail Analysis\n",
    "\n",
    "Now let's analyze what the data tells us about the trails themselves - their characteristics, distribution, and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Spatial Distribution\n",
    "\n",
    "Analyze where the trails are located and their spatial coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze spatial distribution for main trail layer\n",
    "from trails.utils.geo import calculate_lengths_meters\n",
    "\n",
    "hiking_trail_layer = \"hiking_trail_centerline\" if trail_data.language == Language.EN else \"fotrute_senterlinje\"\n",
    "\n",
    "if hiking_trail_layer in trail_data.spatial_layers:\n",
    "    trails = trail_data.spatial_layers[hiking_trail_layer]\n",
    "\n",
    "    print(\"Spatial Distribution Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Get bounds\n",
    "    bounds = trails.total_bounds\n",
    "    print(\"\\nSpatial extent (minx, miny, maxx, maxy):\")\n",
    "    print(f\"  {bounds}\")\n",
    "    print(f\"\\nCRS: {trails.crs}\")\n",
    "\n",
    "    # Analyze by object_type if it exists (translated column name)\n",
    "    objtype_col = \"object_type\" if trail_data.language == Language.EN else \"objtype\"\n",
    "    if objtype_col in trails.columns:\n",
    "        print(f\"\\nDistribution by {objtype_col}:\")\n",
    "        type_stats = trails[objtype_col].value_counts()\n",
    "        for trail_type, count in type_stats.items():\n",
    "            # Values are already expanded and translated\n",
    "            print(f\"  {trail_type}: {count} trails ({count / len(trails) * 100:.1f}%)\")\n",
    "\n",
    "    # Calculate total length in meters (optimized batch calculation)\n",
    "    print(\"\\nLength Analysis:\")\n",
    "    print(\"  Calculating lengths ...\")\n",
    "\n",
    "    # Use the new optimized batch function\n",
    "    trails_copy = trails.copy()\n",
    "    trails_copy[\"length_meters\"] = calculate_lengths_meters(trails_copy)\n",
    "\n",
    "    total_length = trails_copy[\"length_meters\"].sum()\n",
    "    avg_length = trails_copy[\"length_meters\"].mean()\n",
    "    median_length = trails_copy[\"length_meters\"].median()\n",
    "\n",
    "    # Print in kilometers for readability\n",
    "    print(f\"  Total length: {total_length / 1000:,.1f} km\")\n",
    "    print(f\"  Average trail length: {avg_length / 1000:.2f} km\")\n",
    "    print(f\"  Median trail length: {median_length / 1000:.2f} km\")\n",
    "\n",
    "    # Show distribution of trail lengths\n",
    "    print(\"\\nTrail Length Distribution:\")\n",
    "    print(f\"  Shortest trail: {trails_copy['length_meters'].min() / 1000:.3f} km\")\n",
    "    print(f\"  Longest trail: {trails_copy['length_meters'].max() / 1000:.1f} km\")\n",
    "\n",
    "    # Length categories (using meters for calculation, displaying in km)\n",
    "    bins = [0, 1000, 5000, 10000, 20000, float(\"inf\")]\n",
    "    labels = [\"<1km\", \"1-5km\", \"5-10km\", \"10-20km\", \">20km\"]\n",
    "    trails_copy[\"length_category\"] = pd.cut(trails_copy[\"length_meters\"], bins=bins, labels=labels)\n",
    "\n",
    "    print(\"\\nTrails by length category:\")\n",
    "    for category in labels:\n",
    "        count = (trails_copy[\"length_category\"] == category).sum()\n",
    "        pct = count / len(trails_copy) * 100\n",
    "        print(f\"  {category}: {count:,} trails ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Trail Names & Types\n",
    "\n",
    "Analyze trail naming patterns and classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trail names and types analysis\n",
    "from typing import Any\n",
    "\n",
    "print(\"TRAIL NAMES & TYPES ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Trail names\n",
    "print(\"\\n1. TRAIL NAMES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"trail_name\" in hiking_attrs.columns:\n",
    "    name_dtype = hiking_attrs[\"trail_name\"].dtype\n",
    "    unique_names = hiking_attrs[\"trail_name\"].nunique()\n",
    "    total_named = hiking_attrs[\"trail_name\"].notna().sum()\n",
    "    unnamed = hiking_attrs[\"trail_name\"].isna().sum()\n",
    "\n",
    "    print(f\"Column: trail_name [{name_dtype}]\")\n",
    "    print(f\"Unique trail names: {unique_names:,}\")\n",
    "    print(f\"Named trail segments: {total_named:,} ({total_named / len(hiking_attrs) * 100:.1f}%)\")\n",
    "    print(f\"Unnamed segments: {unnamed:,} ({unnamed / len(hiking_attrs) * 100:.1f}%)\")\n",
    "\n",
    "    # Most common trail names\n",
    "    name_counts = hiking_attrs[\"trail_name\"].value_counts()\n",
    "\n",
    "    print(\"\\nTop 15 most common trail names:\")\n",
    "    # Iterate without unpacking to avoid type issues\n",
    "    for _i, item in enumerate(name_counts.head(15).items()):\n",
    "        trail_name_val: Any = item[0]  # Type hint to avoid mypy error\n",
    "        trail_count: int = item[1]\n",
    "        pct = trail_count / len(hiking_attrs) * 100\n",
    "        print(f\"  {trail_name_val}: {trail_count:,} segments ({pct:.2f}%)\")\n",
    "\n",
    "    # Trail name patterns\n",
    "    print(\"\\n2. TRAIL NAME PATTERNS\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Check for numbered trails\n",
    "    numbered_pattern = hiking_attrs[\"trail_name\"].str.contains(r\"\\d+\", na=False).sum()\n",
    "    print(f\"Trails with numbers: {numbered_pattern:,}\")\n",
    "\n",
    "    # Check for common prefixes/patterns\n",
    "    common_patterns = {\n",
    "        \"Pilegrimsleden\": hiking_attrs[\"trail_name\"].str.contains(\"Pilegrimsleden\", na=False).sum(),\n",
    "        \"Kyststi\": hiking_attrs[\"trail_name\"].str.contains(\"Kyststi\", na=False).sum(),\n",
    "        \"DNT\": hiking_attrs[\"trail_name\"].str.contains(\"DNT\", na=False).sum(),\n",
    "        \"Tursti\": hiking_attrs[\"trail_name\"].str.contains(\"Tursti\", na=False).sum(),\n",
    "        \"Rundtur\": hiking_attrs[\"trail_name\"].str.contains(\"Rundtur\", na=False).sum(),\n",
    "    }\n",
    "\n",
    "    print(\"Common trail name patterns:\")\n",
    "    for pattern, pattern_count in common_patterns.items():\n",
    "        if pattern_count > 0:\n",
    "            pct = pattern_count / total_named * 100\n",
    "            print(f\"  Contains '{pattern}': {pattern_count:,} segments ({pct:.2f}%)\")\n",
    "\n",
    "# Trail types\n",
    "print(\"\\n3. TRAIL TYPES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"trail_type\" in hiking_attrs.columns:\n",
    "    type_dtype = hiking_attrs[\"trail_type\"].dtype\n",
    "    type_counts = hiking_attrs[\"trail_type\"].value_counts()\n",
    "\n",
    "    print(f\"Column: trail_type [{type_dtype}]\")\n",
    "    print(\"Trail type distribution:\")\n",
    "    for trail_type, type_count in type_counts.items():\n",
    "        pct = type_count / len(hiking_attrs) * 100\n",
    "        print(f\"  {trail_type}: {type_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Special hiking trail types\n",
    "if \"special_hiking_trail_type\" in hiking_attrs.columns:\n",
    "    print(\"\\n4. SPECIAL TRAIL TYPES\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    special_dtype = hiking_attrs[\"special_hiking_trail_type\"].dtype\n",
    "    special_counts = hiking_attrs[\"special_hiking_trail_type\"].value_counts()\n",
    "\n",
    "    print(f\"Column: special_hiking_trail_type [{special_dtype}]\")\n",
    "    print(\"Special hiking trail classifications:\")\n",
    "    for special_type, special_count in special_counts.items():\n",
    "        pct = special_count / len(hiking_attrs) * 100\n",
    "        print(f\"  {special_type}: {special_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Trail significance\n",
    "if \"trail_significance\" in hiking_attrs.columns:\n",
    "    print(\"\\n5. TRAIL SIGNIFICANCE\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    sig_dtype = hiking_attrs[\"trail_significance\"].dtype\n",
    "    sig_counts = hiking_attrs[\"trail_significance\"].value_counts()\n",
    "\n",
    "    print(f\"Column: trail_significance [{sig_dtype}]\")\n",
    "    print(\"Trail significance levels:\")\n",
    "    for significance, sig_count in sig_counts.items():\n",
    "        pct = sig_count / len(hiking_attrs) * 100\n",
    "        print(f\"  {significance}: {sig_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Object type from spatial layer\n",
    "if \"object_type\" in trails.columns:\n",
    "    print(\"\\n6. OBJECT TYPES (from spatial layer)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    obj_dtype = trails[\"object_type\"].dtype\n",
    "    obj_counts = trails[\"object_type\"].value_counts()\n",
    "\n",
    "    print(f\"Column: object_type [{obj_dtype}]\")\n",
    "    print(\"Object type distribution:\")\n",
    "    for obj_type, obj_count in obj_counts.items():\n",
    "        pct = obj_count / len(trails) * 100\n",
    "        print(f\"  {obj_type}: {obj_count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Difficulty & Classification\n",
    "\n",
    "Analyze trail difficulty ratings and user classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difficulty and classification analysis\n",
    "print(\"DIFFICULTY & CLASSIFICATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Difficulty distribution\n",
    "print(\"\\n1. DIFFICULTY RATINGS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"difficulty\" in hiking_attrs.columns:\n",
    "    diff_counts = hiking_attrs[\"difficulty\"].value_counts()\n",
    "    diff_total = hiking_attrs[\"difficulty\"].notna().sum()\n",
    "\n",
    "    print(f\"Trails with difficulty rating: {diff_total:,} ({diff_total / len(hiking_attrs) * 100:.1f}%)\")\n",
    "    print(f\"Missing difficulty rating: {len(hiking_attrs) - diff_total:,}\")\n",
    "\n",
    "    print(\"\\nDifficulty distribution:\")\n",
    "    for difficulty, count in diff_counts.items():\n",
    "        pct = count / len(hiking_attrs) * 100\n",
    "        pct_rated = count / diff_total * 100\n",
    "        print(f\"  {difficulty}: {count:,} ({pct:.1f}% of all, {pct_rated:.1f}% of rated)\")\n",
    "\n",
    "# Hiking skill level\n",
    "print(\"\\n2. HIKING SKILL LEVEL\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"hiking_skill_level\" in hiking_attrs.columns:\n",
    "    skill_counts = hiking_attrs[\"hiking_skill_level\"].value_counts()\n",
    "    skill_total = hiking_attrs[\"hiking_skill_level\"].notna().sum()\n",
    "\n",
    "    print(f\"Trails with skill level: {skill_total:,} ({skill_total / len(hiking_attrs) * 100:.1f}%)\")\n",
    "\n",
    "    if skill_total > 0:\n",
    "        print(\"\\nSkill level distribution:\")\n",
    "        for skill, count in skill_counts.items():\n",
    "            pct = count / len(hiking_attrs) * 100\n",
    "            pct_rated = count / skill_total * 100\n",
    "            print(f\"  {skill}: {count:,} ({pct:.1f}% of all, {pct_rated:.1f}% of rated)\")\n",
    "\n",
    "# Physical level\n",
    "print(\"\\n3. PHYSICAL LEVEL\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"physical_level\" in hiking_attrs.columns:\n",
    "    phys_counts = hiking_attrs[\"physical_level\"].value_counts()\n",
    "    phys_total = hiking_attrs[\"physical_level\"].notna().sum()\n",
    "\n",
    "    print(f\"Trails with physical level: {phys_total:,} ({phys_total / len(hiking_attrs) * 100:.1f}%)\")\n",
    "\n",
    "    if phys_total > 0:\n",
    "        print(\"\\nPhysical level distribution:\")\n",
    "        for phys, count in phys_counts.items():\n",
    "            pct = count / len(hiking_attrs) * 100\n",
    "            pct_rated = count / phys_total * 100\n",
    "            print(f\"  {phys}: {count:,} ({pct:.1f}% of all, {pct_rated:.1f}% of rated)\")\n",
    "\n",
    "# Cross-analysis: difficulty by trail type\n",
    "if \"difficulty\" in hiking_attrs.columns and \"trail_type\" in hiking_attrs.columns:\n",
    "    print(\"\\n4. DIFFICULTY BY TRAIL TYPE\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Get trails with both difficulty and type\n",
    "    both_fields = hiking_attrs[hiking_attrs[\"difficulty\"].notna() & hiking_attrs[\"trail_type\"].notna()]\n",
    "\n",
    "    if len(both_fields) > 0:\n",
    "        print(f\"Trails with both difficulty and type: {len(both_fields):,}\")\n",
    "\n",
    "        # Create crosstab\n",
    "        cross_tab = pd.crosstab(both_fields[\"trail_type\"], both_fields[\"difficulty\"])\n",
    "\n",
    "        print(\"\\nDifficulty distribution by trail type:\")\n",
    "        for trail_type in cross_tab.index[:5]:  # Show top 5 trail types\n",
    "            print(f\"\\n{trail_type}:\")\n",
    "            row_total = cross_tab.loc[trail_type].sum()\n",
    "            for difficulty in cross_tab.columns:\n",
    "                count = cross_tab.loc[trail_type, difficulty]\n",
    "                if count > 0:\n",
    "                    pct = count / row_total * 100\n",
    "                    print(f\"  {difficulty}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Accessibility features\n",
    "print(\"\\n5. ACCESSIBILITY FEATURES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "accessibility_fields = [\"wheelchair_accessible\", \"stroller_accessible\", \"handicap_standard\"]\n",
    "\n",
    "for field in accessibility_fields:\n",
    "    if field in hiking_attrs.columns:\n",
    "        accessible = hiking_attrs[field].notna().sum()\n",
    "        if accessible > 0:\n",
    "            print(f\"{field}: {accessible:,} trails ({accessible / len(hiking_attrs) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Trail Maintenance & Management\n",
    "\n",
    "Analyze who maintains and manages the trails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trail maintenance and management analysis\n",
    "from typing import Any\n",
    "\n",
    "print(\"TRAIL MAINTENANCE & MANAGEMENT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Maintenance responsibility\n",
    "print(\"\\n1. MAINTENANCE ORGANIZATIONS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"maintenance_responsible\" in hiking_attrs.columns:\n",
    "    maint_counts = hiking_attrs[\"maintenance_responsible\"].value_counts()\n",
    "    maint_total = hiking_attrs[\"maintenance_responsible\"].notna().sum()\n",
    "\n",
    "    print(f\"Trails with maintenance info: {maint_total:,} ({maint_total / len(hiking_attrs) * 100:.1f}%)\")\n",
    "    print(f\"Unique maintenance organizations: {hiking_attrs['maintenance_responsible'].nunique():,}\")\n",
    "\n",
    "    print(\"\\nTop 15 maintenance organizations:\")\n",
    "    # Iterate without unpacking to avoid type issues\n",
    "    for _i, item in enumerate(maint_counts.head(15).items()):\n",
    "        org_name: Any = item[0]  # Type hint to avoid mypy error\n",
    "        org_count: int = item[1]\n",
    "        pct = org_count / len(hiking_attrs) * 100\n",
    "        print(f\"  {org_name}: {org_count:,} trails ({pct:.1f}%)\")\n",
    "\n",
    "    # Check for common organization types\n",
    "    print(\"\\n2. ORGANIZATION TYPES\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    org_patterns = {\n",
    "        \"Kommune\": hiking_attrs[\"maintenance_responsible\"].str.contains(\"kommune|Kommune\", na=False).sum(),\n",
    "        \"DNT\": hiking_attrs[\"maintenance_responsible\"].str.contains(\"DNT|Turistforening\", na=False).sum(),\n",
    "        \"Idrettslag\": hiking_attrs[\"maintenance_responsible\"].str.contains(\"idrettslag|IL|Idrettslag\", na=False).sum(),\n",
    "        \"Turlag\": hiking_attrs[\"maintenance_responsible\"].str.contains(\"turlag|Turlag\", na=False).sum(),\n",
    "        \"Private\": hiking_attrs[\"maintenance_responsible\"].str.contains(\"privat|Privat\", na=False).sum(),\n",
    "    }\n",
    "\n",
    "    print(\"Organization type patterns:\")\n",
    "    for pattern, pattern_total in org_patterns.items():\n",
    "        if pattern_total > 0:\n",
    "            pct = pattern_total / maint_total * 100\n",
    "            print(f\"  {pattern}: {pattern_total:,} trails ({pct:.1f}%)\")\n",
    "\n",
    "# Owner information\n",
    "print(\"\\n3. TRAIL OWNERSHIP\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"owner\" in hiking_attrs.columns:\n",
    "    owner_counts = hiking_attrs[\"owner\"].value_counts()\n",
    "    owner_total = hiking_attrs[\"owner\"].notna().sum()\n",
    "\n",
    "    print(f\"Trails with owner info: {owner_total:,} ({owner_total / len(hiking_attrs) * 100:.1f}%)\")\n",
    "    print(f\"Unique owners: {hiking_attrs['owner'].nunique():,}\")\n",
    "\n",
    "    if owner_total > 0:\n",
    "        print(\"\\nTop 10 trail owners:\")\n",
    "        for owner_name, owner_count in owner_counts.head(10).items():\n",
    "            pct = owner_count / owner_total * 100\n",
    "            print(f\"  {owner_name}: {owner_count:,} trails ({pct:.1f}% of those with owner info)\")\n",
    "\n",
    "# Contractual operator\n",
    "print(\"\\n4. CONTRACTUAL OPERATORS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"contractual_operator\" in hiking_attrs.columns:\n",
    "    operator_counts = hiking_attrs[\"contractual_operator\"].value_counts()\n",
    "    operator_total = hiking_attrs[\"contractual_operator\"].notna().sum()\n",
    "\n",
    "    print(f\"Trails with operator info: {operator_total:,} ({operator_total / len(hiking_attrs) * 100:.1f}%)\")\n",
    "\n",
    "    if operator_total > 0:\n",
    "        print(f\"Unique operators: {hiking_attrs['contractual_operator'].nunique():,}\")\n",
    "        print(\"\\nTop operators:\")\n",
    "        for operator_name, operator_count in operator_counts.head(5).items():\n",
    "            pct = operator_count / operator_total * 100\n",
    "            print(f\"  {operator_name}: {operator_count:,} trails ({pct:.1f}%)\")\n",
    "\n",
    "# Marking and signage (from spatial layer)\n",
    "print(\"\\n5. TRAIL INFRASTRUCTURE\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if \"marking\" in trails.columns:\n",
    "    marked = trails[\"marking\"].notna().sum()\n",
    "    print(f\"Segments with marking info: {marked:,} ({marked / len(trails) * 100:.1f}%)\")\n",
    "\n",
    "    if marked > 0:\n",
    "        mark_values = trails[\"marking\"].value_counts()\n",
    "        for mark_type, mark_count in mark_values.items():\n",
    "            pct = mark_count / len(trails) * 100\n",
    "            print(f\"  {mark_type}: {mark_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "if \"signage\" in trails.columns:\n",
    "    signage = trails[\"signage\"].notna().sum()\n",
    "    print(f\"\\nSegments with signage info: {signage:,} ({signage / len(trails) * 100:.1f}%)\")\n",
    "\n",
    "    if signage > 0:\n",
    "        sign_values = trails[\"signage\"].value_counts()\n",
    "        for sign_type, sign_count in sign_values.items():\n",
    "            pct = sign_count / len(trails) * 100\n",
    "            print(f\"  {sign_type}: {sign_count:,} ({pct:.1f}%)\")\n",
    "\n",
    "if \"lighting\" in trails.columns:\n",
    "    lighting = trails[\"lighting\"].notna().sum()\n",
    "    print(f\"\\nSegments with lighting: {lighting:,} ({lighting / len(trails) * 100:.1f}%)\")\n",
    "\n",
    "    if lighting > 0:\n",
    "        light_values = trails[\"lighting\"].value_counts()\n",
    "        for light_type, light_count in light_values.items():\n",
    "            pct = light_count / len(trails) * 100\n",
    "            print(f\"  {light_type}: {light_count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic visualizations for main trail layer\n",
    "hiking_trail_layer = \"hiking_trail_centerline\" if trail_data.language == Language.EN else \"fotrute_senterlinje\"\n",
    "\n",
    "if hiking_trail_layer in trail_data.spatial_layers:\n",
    "    trails = trail_data.spatial_layers[hiking_trail_layer]\n",
    "\n",
    "    # Plot all trails\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "    # Plot trails with a simple color\n",
    "    trails.plot(ax=ax, linewidth=0.5, alpha=0.6, color=\"darkgreen\")\n",
    "\n",
    "    ax.set_title(f\"Norwegian Hiking Trails ({len(trails)} features)\", fontsize=14)\n",
    "    ax.set_xlabel(\"X Coordinate\")\n",
    "    ax.set_ylabel(\"Y Coordinate\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add CRS info\n",
    "    ax.text(\n",
    "        0.02,\n",
    "        0.98,\n",
    "        f\"CRS: {trails.crs}\",\n",
    "        transform=ax.transAxes,\n",
    "        fontsize=10,\n",
    "        verticalalignment=\"top\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # If we have object type, show distribution\n",
    "    objtype_col = \"object_type\" if trail_data.language == Language.EN else \"objtype\"\n",
    "    if objtype_col in trails.columns:\n",
    "        type_counts = trails[objtype_col].value_counts()\n",
    "\n",
    "        if len(type_counts) > 0:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "            # Bar chart\n",
    "            type_counts.head(10).plot(kind=\"bar\", ax=ax1, color=\"forestgreen\")\n",
    "            ax1.set_title(\"Trail Types Distribution (Top 10)\")\n",
    "            ax1.set_xlabel(\"Trail Type\")\n",
    "            ax1.set_ylabel(\"Count\")\n",
    "            ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "            # Pie chart for top 5\n",
    "            type_counts.head(5).plot(kind=\"pie\", ax=ax2, autopct=\"%1.1f%%\")\n",
    "            ax2.set_title(\"Top 5 Trail Types\")\n",
    "            ax2.set_ylabel(\"\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
